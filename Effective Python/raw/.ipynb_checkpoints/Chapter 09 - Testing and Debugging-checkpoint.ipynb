{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fatal-medium",
   "metadata": {},
   "source": [
    "## Item 75: Use `repr` Strings for Debugging Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-amsterdam",
   "metadata": {},
   "source": [
    "- for most things, all you need to do is call `print` to see how the state of your program changes while it runs to understand where it goes wrong\n",
    "- the `print` function outputs a human-readable string version of whatever you supply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "occupational-trailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo bar\n",
      "foo bar\n",
      "foo bar\n",
      "foo bar\n",
      "foo bar\n",
      "foo bar\n"
     ]
    }
   ],
   "source": [
    "my_value = 'foo bar'\n",
    "print(str(my_value))\n",
    "print('%s' % my_value)\n",
    "print(f'{my_value}')\n",
    "print(format(my_value))\n",
    "print(my_value.__format__('s'))\n",
    "print(my_value.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-printer",
   "metadata": {},
   "source": [
    "- the problem is that human-redable string for a value does not make it clear what the actual type and its specific compositions are\n",
    "- for example\n",
    "- for example `print(5)` will not tell you if its an `int` or a `str`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-phase",
   "metadata": {},
   "source": [
    "- what you almost always want while debugging is to see the `repr` verision of an object\n",
    "- the `repr` built-in function returns the printable representation of an object, which should be its most clearly understandable string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addressed-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\x07'\n"
     ]
    }
   ],
   "source": [
    "a = '\\x07'\n",
    "print(repr(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-scientist",
   "metadata": {},
   "source": [
    "- passing the value from `repr` to the `eval` built-in function should result in the same Python object that you started with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laden-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = eval(repr(a))\n",
    "assert a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-namibia",
   "metadata": {},
   "source": [
    "- when you are debugging with `print` you should call `repr` on a value before printing to ensure that any difference in types is clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "altered-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "'5'\n"
     ]
    }
   ],
   "source": [
    "print(repr(5))\n",
    "print(repr('5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-alexander",
   "metadata": {},
   "source": [
    "- this is equivalent to using `%r` format string with the `%` format string with the `%` operator or an `f-string` with the `!r` type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "consistent-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "'5'\n",
      "5 != '5'\n"
     ]
    }
   ],
   "source": [
    "print('%r' % 5)\n",
    "print('%r' % '5')\n",
    "\n",
    "int_value = 5\n",
    "str_value = '5'\n",
    "print(f'{int_value!r} != {str_value!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-parallel",
   "metadata": {},
   "source": [
    "- for instances of Python classes, the default-readable string value is the same as the `repr` value \n",
    "- this means that passing an instance to `print` will do the right thing, and you dont need to explicitly call `repr` on it\n",
    "- unfortunately, the default implementation of the `repr` for `object` subclass isnt espically helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "correct-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.OpaqueClass object at 0x000001F233B92340>\n"
     ]
    }
   ],
   "source": [
    "class OpaqueClass:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "obj = OpaqueClass(1, 'foo')\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-cleaner",
   "metadata": {},
   "source": [
    "- the output cant be passed to the `eval` function, and it says nothing about the instance fields of the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-tsunami",
   "metadata": {},
   "source": [
    "- there are two solution\n",
    "- you can define your own `__repr__` special method that returns a string containing the Pythone expression that re-creates the objec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pediatric-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetterClass(2, 'bar')\n"
     ]
    }
   ],
   "source": [
    "class BetterClass:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'BetterClass({self.x!r}, {self.y!r})'\n",
    "\n",
    "obj = BetterClass(2, 'bar')\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-merchant",
   "metadata": {},
   "source": [
    "- when you dont have control over the class definition, you can reach into the object instance dictionary, which is stored in the `__dict__` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cardiovascular-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 4, 'y': 'baz'}\n"
     ]
    }
   ],
   "source": [
    "obj = OpaqueClass(4, 'baz')\n",
    "print(obj.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-defensive",
   "metadata": {},
   "source": [
    "## Item 76 Verify Related Behaviors in `TestCase` Subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-grill",
   "metadata": {},
   "source": [
    "- suppose we gave a `utils` function we would like to verify works correctly across a variety of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automatic-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(data):\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    elif isinstance(data, bytes):\n",
    "        return data.decode('utf-8')\n",
    "    else:\n",
    "        raise TypeError('Must supply str or bytes, '\n",
    "                        'found: %r' % data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase, main\n",
    "\n",
    "\n",
    "class UtilsTestCase(TestCase):\n",
    "    def test_to_str_bytes(self):\n",
    "        self.assertEqual('hello', to_str(b'hello'))\n",
    "    \n",
    "    def test_to_str_str(self):\n",
    "        self.assertEqual('hello', to_str('hello'))\n",
    "    \n",
    "    def test_failing(self):\n",
    "        self.assertEqual('incorrect', to_str('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-drunk",
   "metadata": {},
   "source": [
    "- tests are organized into `TestCase` subclasses\n",
    "- each test case is a method beginning with the word `test`\n",
    "- if a test method runs without raising any kind of `Exception` the test is considered to have passed successfully\n",
    "- if one test fails, the `TestCase` subclass continues running the other test methods so you can get a full picture of how all of your tests are doing instead of stopping at the first sign of trouble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-allergy",
   "metadata": {},
   "source": [
    "- if you want to iterate quickly to fix or improve a specific test, you can run only that test method by specifying its path \n",
    "    - ` python3 utils_test.py UtilsTestCase.test_to_str_bytes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-aquarium",
   "metadata": {},
   "source": [
    "- you can also invoke the debugger from directly withing test methods at specifc breakpoints in order to dig more deeply into the cause of failure\n",
    "- `TestCase` class provides helper methods for making assertions in your tests, such as `assertEqual` for verifying equality, `assertTrue` for verifying Boolean expressions and others by typeing `help(TeseCase)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-taylor",
   "metadata": {},
   "source": [
    "- there is also `assertRaises` helper method for verifying exceptions that can be used as a context manager in `with` statements\n",
    "- this appears similiar to a `try/except` statement and makes it abundantly clear where the exception is expected to be raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "described-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilsErrorTestCase(TestCase):\n",
    "    def test_to_str_bad(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            to_str(object())\n",
    "    \n",
    "    def test_to_str_bad_encoding(self):\n",
    "        with self.assertRaises(UnicodeDecodeError):\n",
    "            to_str(b'\\xfa\\xfa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-level",
   "metadata": {},
   "source": [
    "- you can define your own helper methods with compelx logic in `TestCase` subclasses to make your tests more readable\n",
    "- just ensure that the method names dont begin with the word `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-cologne",
   "metadata": {},
   "source": [
    "> helper method makes the test cases short and readable and the outputted error messages are easy to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-proceeding",
   "metadata": {},
   "source": [
    "- it is good to define one `TestCase` subclass for each set of related tests\n",
    "- sometimes you can have one `TestCase` subclass for each function that has many edge cases\n",
    "- other times we create one `TestCase` subclass for testing each basic class and all of its methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-beads",
   "metadata": {},
   "source": [
    "- the `TestCase` class also provides a `subTest` helper method that enables you to avoid boilerplate by defining multiple tests within a single test method\n",
    "- this is especially helpful for writing data-driven tests and allows the test method to continue testing cases even after one oif them fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "settled-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDrivenTestCase(TestCase):\n",
    "    def test_good(self):\n",
    "        good_cases = [\n",
    "            (b'my bytes', 'my bytes'),\n",
    "            ('no error', b'no error'), # This one will fail\n",
    "            ('other str', 'other str'),\n",
    "            ...\n",
    "        ]\n",
    "        for value, expected in good_cases:\n",
    "            with self.subTest(value):\n",
    "                self.assertEqual(expected, to_str(value))\n",
    "\n",
    "    def test_bad(self):\n",
    "        bad_cases = [\n",
    "            (object(), TypeError),\n",
    "            (b'\\xfa\\xfa', UnicodeDecodeError),\n",
    "            ...\n",
    "            ]\n",
    "        for value, exception in bad_cases:\n",
    "            with self.subTest(value):\n",
    "                with self.assertRaises(exception):\n",
    "                    to_str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-spencer",
   "metadata": {},
   "source": [
    "## Item 77: Isolate Tests from Each Other with `setUp`, `tearDown`, `setUpModule` and `tearDownModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-cooperation",
   "metadata": {},
   "source": [
    "- you can override the `setUp` and the `tearDown` methods of `TestCase` subclasses\n",
    "- these methods are called before and after each test method so that you can ensure that each test runs in isolation, which is an important best pratice of proper testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-purple",
   "metadata": {},
   "source": [
    "- below we define a `TestCase` that creates a temporary dictionary before each test and deleted its contents after each test finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "understood-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from unittest import TestCase, main\n",
    "\n",
    "class EnvironmentTest(TestCase):\n",
    "    def setUp(self):\n",
    "        self.test_dir = TemporaryDirectory()\n",
    "        self.test_path = path(self.test_dir.name)\n",
    "        \n",
    "    def tearDown(self):\n",
    "        self.test_dir.cleanup()\n",
    "        \n",
    "    def test_modify_file(self):\n",
    "        with open(self.test_path / 'data.bin', 'w') as f:\n",
    "            ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-smart",
   "metadata": {},
   "source": [
    "- when programs get complicated you want additional tests to verify the end-to-end interactions between your modules instead of only testing code in isolation\n",
    "- this is the difference between `unit tests` and `integration tests`\n",
    "- in python, its important to write both tyes of tests for exactly the same reason: you have no guarantee that your modules will actually work together unless you prove it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-accommodation",
   "metadata": {},
   "source": [
    "- one common problem is that setting up your test enviorment for integration tests can be computationally expensive and may require alot of wall-clock time\n",
    "- for example, say you have to start up the database and tear it down every time\n",
    "- its not praticle to do this for every `setUp` or `tearDown`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-sodium",
   "metadata": {},
   "source": [
    "- `unittest` library provides you ways to configure an expensive resource a single time and then all `TestCase` classes and their test methods run without repeating that initialization\n",
    "- the methods we can use are `setUpModule` and `teardownModule` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "appointed-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase, main\n",
    "\n",
    "def setUpModule():\n",
    "    print('* Module setup')\n",
    "    \n",
    "def tearDownModule():\n",
    "    print('* Module clean-up')\n",
    "    \n",
    "class IntegrationTest(TestCase):\n",
    "    def setUp(self):\n",
    "        print('* Test setup')\n",
    "        \n",
    "    def tearDown(self):\n",
    "        print('* Test clean-up')\n",
    "        \n",
    "    def test_end_to_end1(self):\n",
    "        print('* Test 1')\n",
    "\n",
    "    def test_end_to_end2(self):\n",
    "        print('* Test 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-courage",
   "metadata": {},
   "source": [
    "## Item 78: Use Mocks to Test Code with Complex Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-footwear",
   "metadata": {},
   "source": [
    "- when writting tests, its common to use mocked functions and classes to simulate behavior when its too difficult or slow to use the real thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-mercy",
   "metadata": {},
   "source": [
    "- for example, lets say we need a program to maintain the feeding schedule for animals at the zoo\n",
    "- we define a function to query a database for all the animals of a certain species and return when they most recently ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "english-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConnection:\n",
    "    ...\n",
    "    \n",
    "def get_animals(database, species):\n",
    "    # Query the database\n",
    "    ...\n",
    "    # Return a list of (name, last_mealtime) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-diesel",
   "metadata": {},
   "source": [
    "- to test this we could create a database and populate it with data but that takes alot of wall clock time\n",
    "- its better to use mocks\n",
    "- a `mock` lets you provide expected responses for dependent functions, given a set of expected call\n",
    "- a `mock` is not a `fake`\n",
    "    - a `fake` would provide most of the behavior of the `DatabaseConnection` class but with a simpler implementation, such as basic in-memory, single-threaded database with no persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-easter",
   "metadata": {},
   "source": [
    "- python has the `unittest.mock` built-in module for creating mocks and using them in tests\n",
    "- here we define a `Mock` instance that simulates the `get_animals` functions without actually connecting to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fitting-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from unittest.mock import Mock\n",
    "\n",
    "mock = Mock(spec=get_animals)\n",
    "expected = [\n",
    "    ('Spot', datetime(2019, 6, 5, 11, 15)),\n",
    "    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n",
    "    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n",
    "]\n",
    "mock.return_value = expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-voltage",
   "metadata": {},
   "source": [
    "- the `Mock` class creates a mock function\n",
    "- the `return_value` attribute of the mock is the value to return when it is called\n",
    "- the `spec` argument indicates that the mock should act like the given object, which is a function in this case, and error if its used in the wrong way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-weather",
   "metadata": {},
   "source": [
    "- if we try to trat the mock function ad if it were a mock object with attributes, we get errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "hispanic-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock object has no attribute 'does_not_exist'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mock.does_not_exist\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-dispute",
   "metadata": {},
   "source": [
    "- once its created we can call the mock, get its return value and verify that what it returns matches expecations\n",
    "- we use a unique object value as the `database` argument because it wont actually be used by the mock to do anything\n",
    "- all we care about is that the `database` parameter was correctly plumbed through to any dependent functions that needed a `DatabaseConenction` instance in other work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "veterinary-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = object()\n",
    "result = mock(database, 'Meerkat')\n",
    "assert result == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-ghost",
   "metadata": {},
   "source": [
    "- the code snippet above verifies that the mock responed correctly, but how do we know if the code that called the mock provided the correct arguments?\n",
    "- for this the `Mock` class provides the `assert_called_once_with` method which verifies that a signle call with exactly the given parameters was made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "noticed-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 'mock' to be called once. Called 2 times.\n",
      "Calls: [call(<object object at 0x000001E074F8BF30>, 'Meerkat'),\n",
      " call(<object object at 0x000001E074F8BF40>, 'Meerkat')].\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mock.assert_called_once_with(database, 'Meerkat')\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-walter",
   "metadata": {},
   "source": [
    "- if we supply the wrong parameters, an expection is raised and any `TestCase` that the assertion in used in fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "second-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 'mock' to be called once. Called 2 times.\n",
      "Calls: [call(<object object at 0x000001E074F8BF30>, 'Meerkat'),\n",
      " call(<object object at 0x000001E074F8BF40>, 'Meerkat')].\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mock.assert_called_once_with(database, 'Giraffe')\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-realtor",
   "metadata": {},
   "source": [
    "- if you dont care about some individual parameters, such as exactly which `database` object was used, then we can indicate that any value is okay for an argument by using the `unittest.mock.ANY` constant\n",
    "- we can also use the `assert-called_with` method of `Mock` to verify that the most recent call to the mock- and there may have been multiple calls in this case matches my expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aerial-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import ANY\n",
    "\n",
    "try:\n",
    "    mock = Mock(spec=get_animals)\n",
    "    mock('database 1', 'Rabbit')\n",
    "    mock('database 2', 'Bison')\n",
    "    mock('database 3', 'Meerkat')\n",
    "    mock.assert_called_with(ANY, 'Meerkat')\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-strand",
   "metadata": {},
   "source": [
    "- `ANY` is useful in tests when a parameter is not core to the behavior thats being tested\n",
    "- its often work erring on the side of under-specifying tests used by `ANY` more liberally instead of over-specifying tests and having to plumb through various test parameter expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-france",
   "metadata": {},
   "source": [
    "- the `Mock` class also makes it easy to mock exceptions being raised\n",
    "- all you have to do is use `side_effect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "incorrect-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whoops! Big problem\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "class MyError(Exception):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    mock = Mock(spec=get_animals)\n",
    "    mock.side_effect = MyError('Whoops! Big problem')\n",
    "    result = mock(database, 'Meerkat')\n",
    "except MyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-rider",
   "metadata": {},
   "source": [
    "- use `help(unittest.mock.Mock)` to learn more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-yellow",
   "metadata": {},
   "source": [
    "- below is an example of how to apply `Mock` to actual testing situations to show how to use it effectively in writing unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "affiliated-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_period(database, species):\n",
    "    # Query the database\n",
    "    ...\n",
    "    # Return a time delta\n",
    "    \n",
    "def feed_animal(database, name, when):\n",
    "    # Write to the database\n",
    "    ...\n",
    "    \n",
    "def do_rounds(database, species):\n",
    "    now = datetime.datetime.utcnow()\n",
    "    feeding_timedelta = get_food_period(database, species)\n",
    "    animals = get_animals(database, species)\n",
    "    fed = 0\n",
    "    \n",
    "    for name, last_mealtime in animals:\n",
    "        if (now - last_mealtime) > feeding_timedelta:\n",
    "            feed_animal(database, name, now)\n",
    "            fed += 1\n",
    "    return fed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-french",
   "metadata": {},
   "source": [
    "- the foal of my testr is to verify that when `do_rounds` is run, the right animals get fed, the latest feeding time was recorded to the database, and the total number of animals fed returned by the function matches the correct total\n",
    "- to do all this, we need to mock out `datetime.utcnow` so my tests have a stable time that isn't affected by daylight saving time and other ephermeral changes\n",
    "- we need to mock out `get_food_period` and `get_animals` to return values that would hae come from the database\n",
    "- all we need to mock out `feed_animal` to accept data that would have been written back to the database\n",
    "- we also need to mock out `feed_animal` to accept data that would have been written back to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-compilation",
   "metadata": {},
   "source": [
    "- the problem is even if we know how to create these mock functions and set expectations, how do we get the `do_round` function thats being tested to use the mock dependent functions instead of the real versions?\n",
    "- one approach is to inject everything as keyword-only arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "rolled-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rounds(database, species, *,\n",
    "                now_func=datetime.utcnow,\n",
    "                food_func=get_food_period,\n",
    "                animals_func=get_animals,\n",
    "                feed_func=feed_animal):\n",
    "\n",
    "    now = now_func()\n",
    "    feeding_timedelta = food_func(database, species)\n",
    "    animals = animals_func(database, species)\n",
    "    fed = 0\n",
    "\n",
    "    for name, last_mealtime in animals:\n",
    "        if (now - last_mealtime) > feeding_timedelta:\n",
    "            feed_func(database, name, now)\n",
    "            fed += 1\n",
    "    \n",
    "    return fed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-position",
   "metadata": {},
   "source": [
    "- to test this function, I need to create all of the `Mocks` instances upfront and set their expectantions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "worthy-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "now_func = Mock(spec=datetime.utcnow)\n",
    "now_func.return_value = datetime(2019, 6, 5, 15, 45)\n",
    "\n",
    "food_func = Mock(spec=get_food_period)\n",
    "food_func.return_value = timedelta(hours=3)\n",
    "\n",
    "animals_func = Mock(spec=get_animals)\n",
    "animals_func.return_value = [\n",
    "    ('Spot', datetime(2019, 6, 5, 11, 15)),\n",
    "    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n",
    "    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n",
    "]\n",
    "\n",
    "feed_func = Mock(spec=feed_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-johns",
   "metadata": {},
   "source": [
    "- then we can run the test by passing the mocks into the `do_rounds` function to override the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "nasty-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = do_rounds(\n",
    "    database,\n",
    "    'Meerkat',\n",
    "    now_func=now_func,\n",
    "    food_func=food_func,\n",
    "    animals_func=animals_func,\n",
    "    feed_func=feed_func)\n",
    "\n",
    "assert result == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-option",
   "metadata": {},
   "source": [
    "- finally we can verify that all the calls to dependent functions matched our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "molecular-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import call\n",
    "\n",
    "try: \n",
    "    food_func.assert_called_once_with(database, 'Meerkat')\n",
    "    \n",
    "    animals_func.assert_called_once_with(database, 'Meerkat')\n",
    "    \n",
    "    feed_func.assert_has_calls(\n",
    "    [\n",
    "        call(database, 'Spot', now_func.return_value),\n",
    "        call(database, 'Fluffy', now_func.return_value),\n",
    "    ],\n",
    "    any_order=True)\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-endorsement",
   "metadata": {},
   "source": [
    "- we dont verify the parameters to the `datetime.utcnow` mock or how many times it was called because its indirectly verified by the return value of the function\n",
    "- for `get_food_period` and `get_animals`, we verify a single call with the specified parameters by using `assert_called_once_with`\n",
    "- for the `feed_animal` function we verify that two calls were amade- and their order didn't matter- to write to the database using the `unittest.mock.call` helper and the `assert_has_calls` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-ballet",
   "metadata": {},
   "source": [
    "- this approach using keyword-only arguments for injecting mocks works, buits its very verbose and requires changing every function you want to test\n",
    "- the `unittest.mock.patch` family of functions makes injecting mockes easier\n",
    "- it temporarily reassigns an attribute of a module or class, such as the database-accessing functions that we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-logging",
   "metadata": {},
   "source": [
    "- for example, here we can override `get_animals` to be a mock using patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "chinese-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside patch: <function get_animals at 0x000001E075FAFF70>\n",
      "Inside patch:  <MagicMock name='get_animals' id='2063563728496'>\n",
      "Outside again: <function get_animals at 0x000001E075FAFF70>\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "print('Outside patch:', get_animals)\n",
    "\n",
    "with patch('__main__.get_animals'):\n",
    "    print('Inside patch: ', get_animals)\n",
    "\n",
    "print('Outside again:', get_animals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-plumbing",
   "metadata": {},
   "source": [
    "- patch works for modules, classes and attributes\n",
    "- it can be used in `with` statements, as a functional `decorator` or in the `setUp` and `tearDown` methods of TestCase classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-spare",
   "metadata": {},
   "source": [
    "- `patch` doesent work in all cases\n",
    "- to test `do_rounds` we need to mock out the current time returned by the `datetime.utcnow` class method\n",
    "- python wont let me do this because the `datetime` class is defined in a `C`-extension module, which can't be modified in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "damaged-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't set attributes of built-in/extension type 'datetime.datetime'\n"
     ]
    }
   ],
   "source": [
    "fake_now = datetime(2019, 6, 5, 15, 45)\n",
    "\n",
    "try:\n",
    "    with patch('datetime.datetime.utcnow'):\n",
    "        datetime.utcnow.return_value = fake_now\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-knowing",
   "metadata": {},
   "source": [
    "- to work around this, we can create a helper function to fetch time that can be patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "collective-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_do_rounds_time():\n",
    "    return datetime.datetime.utcnow()\n",
    "def do_rounds(database, species):\n",
    "    now = get_do_rounds_time()\n",
    "    ...\n",
    "\n",
    "with patch('__main__.get_do_rounds_time'):\n",
    "    ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-provider",
   "metadata": {},
   "source": [
    "- alternatively we can use a keyword-only argument for the `datetime.utcnow` mock and use `patch` for all of the other mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "amateur-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rounds(database, species, *, utcnow=datetime.utcnow):\n",
    "    now = utcnow()\n",
    "    feeding_timedelta = get_food_period(database, species)\n",
    "    animals = get_animals(database, species)\n",
    "    fed = 0\n",
    "    \n",
    "    for name, last_mealtime in animals:\n",
    "        if (now - last_mealtime) > feeding_timedelta:\n",
    "            feed_func(database, name, now)\n",
    "            fed += 1\n",
    "    \n",
    "    return fed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-timothy",
   "metadata": {},
   "source": [
    "- we are going to go with the latter approach\n",
    "- now we can use the `patch.multiple` function to create many mocks and set their expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "industrial-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import DEFAULT\n",
    "with patch.multiple('__main__',\n",
    "                    autospec=True,\n",
    "                    get_food_period=DEFAULT,\n",
    "                    get_animals=DEFAULT,\n",
    "                    feed_animal=DEFAULT):\n",
    "\n",
    "    now_func = Mock(spec=datetime.utcnow)\n",
    "    now_func.return_value = datetime(2019, 6, 5, 15, 45)\n",
    "    get_food_period.return_value = timedelta(hours=3)\n",
    "    get_animals.return_value = [\n",
    "        ('Spot', datetime(2019, 6, 5, 11, 15)),\n",
    "        ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n",
    "        ('Jojo', datetime(2019, 6, 5, 12, 45))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-airport",
   "metadata": {},
   "source": [
    "- witht he setup ready, we can run the test and verify that the calls were correct inside the `with` statement that used `patch.multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "announced-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = do_rounds(database, 'Meerkat', utcnow=now_func)\n",
    "    assert result == 2\n",
    "    \n",
    "    food_func.assert_called_once_with(database, 'Meerkat')\n",
    "    animals_func.assert_called_once_with(database, 'Meerkat')\n",
    "    feed_func.assert_has_calls(\n",
    "        [\n",
    "            call(database, 'Spot', now_func.return_value),\n",
    "            call(database, 'Fluffy', now_func.return_value),\n",
    "        ],\n",
    "    any_order=True)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-process",
   "metadata": {},
   "source": [
    "- when the setup is ready, we can run the test and veify that the calls were correct inside the `with` statement that used `patch.multiple`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-description",
   "metadata": {},
   "source": [
    "- the keyword arguments to `patch.multiple` correspond to the names in the `__main__` module that we want to overreide during the test\n",
    "- the `DEFAULT` value indicated that I want to standard `Mock` instance to be created for each name\n",
    "- All of the generated mocks will adhere to the specification of the object they are meant to simultate, thanks to the `autospec=True` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-gates",
   "metadata": {},
   "source": [
    "### Things to Remember"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-scale",
   "metadata": {},
   "source": [
    "- the `unittest.mock` module provides a way to simulate the behavior of interfaces using the `Mock` class\n",
    "- Mocks are useful in tests when its difficult to set up the dependencies that are required by the code that's being tested\n",
    "- when using mocks, it's important to verify both the behavior of the code being tested and how dependent functions were called by that code, using the `Mock.assert_called_once_with` family of methods\n",
    "- keyword-only arguments and the `unittest.mock.patch` family of functions can be used to inject into the code being tested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-profit",
   "metadata": {},
   "source": [
    "## Item 79: Encapsulate Dependencies to Facilitate Mocking and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-explosion",
   "metadata": {},
   "source": [
    "- one way to improve these tests is to use  a wrapper object to encapsulate the database's interface instead of passing a `DatabaseConnection` object to the functions as an argument\n",
    "- its woth refactoring your code to use better abstractions because it facilitates creating mocks and writing tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-shock",
   "metadata": {},
   "source": [
    "- below we define the various database helper functions from the previous item as methods on a class instead of as independent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "unlikely-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZooDatabase:\n",
    "    ...\n",
    "    \n",
    "    def get_animals(self, species):\n",
    "        ...\n",
    "        \n",
    "    def get_food_period(self, species):\n",
    "        ...\n",
    "    \n",
    "    def feed_animal(self, name, when):\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-authority",
   "metadata": {},
   "source": [
    "- now we can redefine the `do_rounds` function to call method son a `ZooDatabase` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "needed-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def do_rounds(database, species, *, utcnow=datetime.utcnow):\n",
    "    now = utcnow()\n",
    "    feeding_timedelta = database.get_food_period(species)\n",
    "    animals = database.get_animals(species)\n",
    "    fed = 0\n",
    "\n",
    "    for name, last_mealtime in animals:\n",
    "        if (now - last_mealtime) >= feeding_timedelta:\n",
    "            database.feed_animal(name, now)\n",
    "            fed += 1\n",
    "    \n",
    "    return fed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-dream",
   "metadata": {},
   "source": [
    "- writing a test for `do_rounds` is now alot easier because we no longer need to use `unittest.mock.patch` to inject the mock into code being tested\n",
    "- instead we can create a `Mock` instance to represent a `ZooDatabase` and pass that in as the database parameter\n",
    "- the `Mock` class returns a mock object for any attribute name that is accessed\n",
    "- those attributes can be called like methods, which we can then use to set expectations and verify calls\n",
    "- this makes it easy to mock out all of the methods of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "herbal-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Mock name='mock.feed_animal' id='2063563903328'>\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import Mock\n",
    "\n",
    "database = Mock(spec=ZooDatabase)\n",
    "print(database.feed_animal)\n",
    "database.feed_animal()\n",
    "database.feed_animal.assert_any_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-poison",
   "metadata": {},
   "source": [
    "- we can rewrite the `Mock` setup code by using the `Zoodatabase` encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "instructional-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from unittest.mock import call\n",
    "\n",
    "now_func = Mock(spec=datetime.utcnow)\n",
    "now_func.return_value = datetime(2019, 6, 5, 15, 45)\n",
    "\n",
    "database = Mock(spec=ZooDatabase)\n",
    "database.get_food_period.return_value = timedelta(hours=3)\n",
    "database.get_animals.return_value = [\n",
    "        ('Spot', datetime(2019, 6, 5, 11, 15)),\n",
    "        ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n",
    "        ('Jojo', datetime(2019, 6, 5, 12, 55))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-database",
   "metadata": {},
   "source": [
    "- then we can returnt he function being tested and verify that all dependent methods were called as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "flush-adoption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 'get_food_period' to be called once. Called 4 times.\n",
      "Calls: [call('Meerkat'), call('Meerkat'), call('Meerkat'), call('Meerkat')].\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = do_rounds(database, 'Meerkat', utcnow=now_func)\n",
    "    assert result == 2\n",
    "\n",
    "    database.get_food_period.assert_called_once_with('Meerkat')\n",
    "    database.get_animals.assert_called_once_with('Meerkat')\n",
    "    database.feed_animal.assert_has_calls(\n",
    "     [\n",
    "         call('Spot', now_func.return_value),\n",
    "         call('Fluffy', now_func.return_value),\n",
    "     ],\n",
    "     any_order=True)\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-experience",
   "metadata": {},
   "source": [
    "- using the `spec` parameter to `Mock` is especially useful when mocking classes because it ensures that the code under test doesnt call a misspelled method name by accident\n",
    "- this allows you to avoid a common pitfall where the same bug is present in both the code and the unit test, masking a real error that will reveal itself in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "important-hampshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock object has no attribute 'bad_method_name'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    database.bad_method_name()\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-replica",
   "metadata": {},
   "source": [
    "- if we want to test this program `end-to-end` with a mid-level integration `test`, we still need a way to inject a mock `ZooDatabase  into the program\n",
    "- we can do this by creating a helper function that acts as a seam for `dependency injection`\n",
    "- here we define a helper function that caches a `ZooDatabase` in module scope using a `global` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "korean-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = None\n",
    "\n",
    "def get_database():\n",
    "    global DATABASE\n",
    "    if DATABASE is None:\n",
    "        DATABASE = ZooDatabase()\n",
    "    return DATABASE\n",
    "\n",
    "def main(argv):\n",
    "    database = get_database()\n",
    "    species = argv[1]\n",
    "    count = do_rounds(database, species)\n",
    "    print(f'Fed {count} {species}(s)')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-herald",
   "metadata": {},
   "source": [
    "- Now we can inject the mock `ZooDatabase` using `patch`, run the test and verify the programs output\n",
    "- were not using a mock `datetime.utcnow` but relying on the database records returned by the mock to be relative to the current time in order to produce similar behavior to the unit test\n",
    "- this approach is more flaky than mocking everything but it also tests more surface area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "seventh-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import contextlib\n",
    "from unittest.mock import patch\n",
    "\n",
    "with patch('__main__.DATABASE', spec=ZooDatabase):\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    DATABASE.get_food_period.return_value = timedelta(hours=3)\n",
    "    DATABASE.get_animals.return_value = [\n",
    "        ('Spot', now - timedelta(minutes=4.5)),\n",
    "        ('Fluffy', now - timedelta(hours=3.25)),\n",
    "        ('Jojo', now - timedelta(hours=3)),\n",
    "    ]\n",
    "    \n",
    "    fake_stdout = io.StringIO()\n",
    "    with contextlib.redirect_stdout(fake_stdout):\n",
    "        main(['program name', 'Meerkat'])\n",
    "    \n",
    "    found = fake_stdout.getvalue()\n",
    "    expected = 'Fed 2 Meerkat(s)\\n'\n",
    "    \n",
    "    assert found == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-windows",
   "metadata": {},
   "source": [
    "- creating this integration test was straightforward because we designed the implmentation to make it easier to test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-clinic",
   "metadata": {},
   "source": [
    "### Things to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-logic",
   "metadata": {},
   "source": [
    "- when unit test require alot of boilerplate to set up mocks, one solution may be to encapsulate the functionality of dependencies into classes that are more easily mocked\n",
    "- the `Mock` class of the `unittest.mock` built-in module simulates classes by returning a new mock, which can act as a mock method, for each atttribute then is accessed\n",
    "- for `end-to-end` tests, its valuable to refactor your code to have more helper function that can act as explicit seams for injecting mock dependencies in tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-smile",
   "metadata": {},
   "source": [
    "## Item 80: Consider Interactive Debugging with `pdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-newspaper",
   "metadata": {},
   "source": [
    "- in Python, the easiest way to use the debugger is by modifying your program to directly initiate the debugger just before you think you'll have an issue worth investigating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-disclosure",
   "metadata": {},
   "source": [
    "- to initiate the debugger, all you have to do is call the `breakpoint` built-in function\n",
    "- this is equivalent to importing the `pdb` module and running its `set_trace` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "taken-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5291502622129182\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_rmse(observed, ideal):\n",
    "    total_err_2 = 0\n",
    "    count = 0\n",
    "    \n",
    "    for got, wanted in zip(observed, ideal):\n",
    "        err_2 = (got - wanted) ** 2\n",
    "        #breakpoint() # Start the debugger here\n",
    "        total_err_2 += err_2\n",
    "        count += 1\n",
    "    \n",
    "    mean_err = total_err_2 / count\n",
    "    rmse = math.sqrt(mean_err)\n",
    "    return rmse\n",
    "\n",
    "result = compute_rmse(\n",
    "    [1.8, 1.7, 3.2, 6],\n",
    "    [2, 1.5, 3, 5])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-mission",
   "metadata": {},
   "source": [
    "- at the `Pdb` prompt you can type in the names of local variables to see their values printed out (or use `p <name>`)\n",
    "- you can see a list of all local variables by calling the `locals` built-in function\n",
    "- you can import modules, inspect global state, construct new objects, run the `help` function and even modify parts of the running program- whatever you need to do to aid in your debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-chamber",
   "metadata": {},
   "source": [
    "- three very useful commands make inspecting the running program easier\n",
    "    - `where`: Print the current execution call stack. this lets you figure out where you are in your program and how you arrived at the `breakpoint` trigger\n",
    "    - `up`: move your scope up the execution call stack to the caller of the current function. this allows you to inspect the local variables in higher levels of the progra that led to the breakpoint\n",
    "    - `down`: move your scope back down the execution call stack one level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-passenger",
   "metadata": {},
   "source": [
    "- when your done inspecting the current state, you can use these five debugger commands to control the programs execution:\n",
    "    - `step`: Run the program unitll the next line of execution in the program, and then return control back to the debugger prompt. If the next line of execution includes calling a function, the debugger stops within the function that was called\n",
    "    - `next`: Run the program untill the next line of execution in the current function, and then return control back to the debugger prompt. If the next line of execution includes calling a function, the debugger will not stop untill the called function has returned\n",
    "    - `return`: Run the program untill the current function returns and then returns control back to the debugger prompt\n",
    "    - `continue`: Continue running the prompt untill the next `breakpoint` call or one added by a debugger command\n",
    "    - `quit`: Exit the debugger and end the program. Run tis command if you've found the problem gone too far or need to make the program modifications and try again "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-party",
   "metadata": {},
   "source": [
    "- `post-mortem debugging` is a useful way to reach the debugger prompt \n",
    "- this enables us to debug a program after its already raised an exception and c rashed\n",
    "- this is helpful when you dont know where to put the break point\n",
    "    - `python3 -m pdb -c continue postmortem_breakpoint.py `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-aside",
   "metadata": {},
   "source": [
    "- you can also use post-mortem debugging after hitting an uncaught exception in the interactive Python interpreter by calling the `pm` function of the `pdb` module\n",
    "\n",
    "        >>> import my_module\n",
    "        >>> my_module.compute_stddev([5])\n",
    "        Traceback(...)\n",
    "        ...\n",
    "        >>> `import pdb; pdb.pm())`\n",
    "        >>> (pdb) err_2_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-spray",
   "metadata": {},
   "source": [
    "### Things to Remember"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-desire",
   "metadata": {},
   "source": [
    "- The pdb module can be used for debug exceptions after they happen in independent Python programs (using `python -m pdb -c continue <program path>`) or the interactive Python interpreter (using import `pdb; pdb.pm()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-clinic",
   "metadata": {},
   "source": [
    "## Item 81: Use `tracemalloc` to Understand Memory Usage and Leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-theta",
   "metadata": {},
   "source": [
    "- memory management in the default implementation of Python, `CPython`, uses refrence counting\n",
    "- this ensures that as soon as all refrences to an object have expired, the referenced object is also cleared from memory, freeing up that space for other data\n",
    "- `CPython` also has a built-in cycle detector to ensure that self-referencing objects are eventually garbage collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-operations",
   "metadata": {},
   "source": [
    "- you generally dont have to worry about memeory in a python application but in pratice you can run out of memeory due to no longer useful refrences still being held"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-death",
   "metadata": {},
   "source": [
    "- the first way to debug memory usage is to ask the `gc` built in module to list every object currently known by the garbace collector\n",
    "- the tool is blunt but leys you quickly get a sense of where your programs memory is bing used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "russian-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waste_memory.py\n",
    "import os\n",
    "class MyObject:\n",
    "    def __init__(self):\n",
    "        self.data = os.urandom(100)\n",
    "    \n",
    "def get_data():\n",
    "    values = []\n",
    "    for _ in range(100):\n",
    "        obj = MyObject()\n",
    "        values.append(obj)\n",
    "    return values\n",
    "\n",
    "def run():\n",
    "    deep_values = []\n",
    "    for _ in range(100):\n",
    "        deep_values.append(get_data())\n",
    "    return deep_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-tower",
   "metadata": {},
   "source": [
    "- then we run a program that uses the `gc` built-in module to print out how many objects were created during execution along with a sample of allocatged objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "previous-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 55746\n"
     ]
    }
   ],
   "source": [
    "# using_gc.py\n",
    "import gc\n",
    "\n",
    "found_objects = gc.get_objects()\n",
    "print('Before:', len(found_objects))\n",
    "\n",
    "# hold_reference = waste_memory.run()\n",
    "\n",
    "# found_objects = gc.get_objects()\n",
    "# print('After: ', len(found_objects))\n",
    "# for obj in found_objects[:3]:\n",
    "#     print(repr(obj)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-paste",
   "metadata": {},
   "source": [
    "- the problem with the `gc.get_objects` is that it does not tell you anything about how the objects were allocated\n",
    "- `tracemalloc` built-in module helps us identify the code responsible for allocating the objects that were leaking memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-medium",
   "metadata": {},
   "source": [
    "- `tracemalloc` makes it possible to connect an object back to where it was allocated\n",
    "- you use it by taking befor and after snapshots of memory usage and comparing them to see whats changed\n",
    "- we use this approach to print out the top three memory usage offenders in our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "characteristic-supplier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9260f813853c>:5: size=2314 KiB (+2314 KiB), count=30000 (+30000), average=79 B\n",
      "<ipython-input-13-9260f813853c>:10: size=469 KiB (+469 KiB), count=10000 (+10000), average=48 B\n",
      "<ipython-input-13-9260f813853c>:11: size=84.4 KiB (+84.4 KiB), count=100 (+100), average=864 B\n"
     ]
    }
   ],
   "source": [
    "# top_n.py\n",
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start(10)                          # Set stack depth\n",
    "time1 = tracemalloc.take_snapshot()            # Before snapshot\n",
    "                                                                               \n",
    "                                                \n",
    "x = run()                                      # Usage to debug\n",
    "time2 = tracemalloc.take_snapshot()            # After snapshot\n",
    "                                                  \n",
    "stats = time2.compare_to(time1, 'lineno')      # Compare snapshots\n",
    "for stat in stats[:3]:\n",
    "    print(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-knife",
   "metadata": {},
   "source": [
    "- the size and count labels in the output make it immediately clear which objects are dominating my projects memory usage and where in the source code they were allocated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-newfoundland",
   "metadata": {},
   "source": [
    "- `tracemalloc` module can also print out the full stack trace of each allocation (up to the number of frames passed to the `tracemalloc.start` function\n",
    "- here i print out the stack trace of the biggest source of memeory usage in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "superior-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest offender is:\n",
      "  File \"c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940\n",
      "    return runner(coro)\n",
      "  File \"c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68\n",
      "    coro.send(None)\n",
      "  File \"c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-18-d63433c329f2>\", line 7\n",
      "    x = run()\n",
      "  File \"<ipython-input-13-9260f813853c>\", line 17\n",
      "    deep_values.append(get_data())\n",
      "  File \"<ipython-input-13-9260f813853c>\", line 10\n",
      "    obj = MyObject()\n",
      "  File \"<ipython-input-13-9260f813853c>\", line 5\n",
      "    self.data = os.urandom(100)\n"
     ]
    }
   ],
   "source": [
    "# with_trace.py\n",
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start(10)\n",
    "time1 = tracemalloc.take_snapshot()\n",
    "\n",
    "x = run()\n",
    "time2 = tracemalloc.take_snapshot()\n",
    "\n",
    "stats = time2.compare_to(time1, 'traceback')\n",
    "top = stats[0]\n",
    "print('Biggest offender is:')\n",
    "print('\\n'.join(top.traceback.format()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-actress",
   "metadata": {},
   "source": [
    "### Things to Remember"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-supplement",
   "metadata": {},
   "source": [
    "- it can be difficult to understand how Python programs use the leak memory\n",
    "- the `gc` module can help you understand which object exist, but it has no information about how they were allocated\n",
    "- the `tracemalloc` built-in module provides powerful tools for understanding the source of memeory usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

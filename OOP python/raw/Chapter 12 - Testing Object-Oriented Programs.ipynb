{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introductory-brunei",
   "metadata": {},
   "source": [
    "## Why Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-drove",
   "metadata": {},
   "source": [
    "- testing might be more important in python then `c++` because of dynamic typing\n",
    "- automated tests automatically run certain inputs through other programs or parts of programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-married",
   "metadata": {},
   "source": [
    "__Four Principles of Maintainable Code__:\n",
    "- ensures that code is working the way the developer thinks it should\n",
    "- ensures that code continues working when we make changes\n",
    "- ensure that developer understood the requirements\n",
    "- ensure that the code we are writting has a maintainable interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-nepal",
   "metadata": {},
   "source": [
    "## Test-driven development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-spank",
   "metadata": {},
   "source": [
    "- this is the _write tests first_ before code ideology\n",
    "- test-driven development takes _untested code is broken code_ concept a step further by saying  that only unwritten code should be untested\n",
    "- we dont write any code untill we have the test that will prove it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-definition",
   "metadata": {},
   "source": [
    "- the first goal of test driven metholodgy is to ensure that the tests reall get written\n",
    "- secondly, writing tests first forces us to consider exactly how the code will be used\n",
    "    - it tells us what methods objects need to have and how attributes will be accessed\n",
    "    - it helps us break up the initial problem into smaller, testable problems and then to recombine the tested solutions into larger, also testes solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-compatibility",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-teens",
   "metadata": {},
   "source": [
    "- this is pythons built in test libary\n",
    "- `unit tests` focus on testing the least amount of code possible in any one test\n",
    "- the most important tool is the `TestCase` class\n",
    "- this class provides a set of methods that allow us to compare values, set up tests and clean up when they have finished\n",
    "- when we want to write a set of unit tests for a specific task, we create a subclass of `TestCase` and write individual methods to do the actual testing\n",
    "- these methods must all start with the name `test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class CheckNumbers(unittest.TestCase):\n",
    "    def test_int_float(self):\n",
    "        self.assertEqual(1, 1.0)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-monkey",
   "metadata": {},
   "source": [
    "- the code above simply subclasses the `TestCase` class and adds a method that calls the `TestCase.assertEqual` method\n",
    "- as long as each method begins with `test`, we can write as many tests as we want\n",
    "- each test howerver should only do one thing\n",
    "- good testing requires keeping each test method as short as possible, testing a small unit of code with each test case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-newfoundland",
   "metadata": {},
   "source": [
    "## Assertion Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-baseline",
   "metadata": {},
   "source": [
    "- the general layout of a test case is to set certain variables to know values, run one or more functions, methods or processes and then _prove_ that correct expected results were returned or calculated by using `TestCase` assertion methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-contribution",
   "metadata": {},
   "source": [
    "- the most common assertion methods `assertEqual` or `assertNotEqual` will not test boolean values\n",
    "- the `assertRaises` method can be used to ensure that specific function call raises a specific exception or, optionally, it can be used as a context manager to wrap inline code\n",
    "- the test passes if the code inside the `with` statement raises the proper exception, otherwise it fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-paragraph",
   "metadata": {},
   "source": [
    "- the context manager allows us to write the code the way we would normally write it (by calling functions or executing core directly), rather than having to wrap the function call in another function call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-store",
   "metadata": {},
   "source": [
    "![](images/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-belarus",
   "metadata": {},
   "source": [
    "## Reducing Boilerplate and cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-wisdom",
   "metadata": {},
   "source": [
    "- we can use `setUp` method on the `TestCase` class to perform initialization for each test \n",
    "- the `setUp` or `tearDown` methods do not need to be called inside of our methods, but they always run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-worker",
   "metadata": {},
   "source": [
    "## Organizing and running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-skill",
   "metadata": {},
   "source": [
    "- we should divide our test `classes` into modules and packages that keep them organized\n",
    "- if we name each test module starting with `test`, there is an easy way to find and run them all\n",
    "- pythons `discover` module looks for any modules in the current folder or `subfolders` with names that start with `test`\n",
    "- if it finds any `TestCase` objects in these modules, the tests are executed\n",
    "- to use this feature name your file `test_<something>.py`\n",
    "- then run `python3-munittestdiscover`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-visiting",
   "metadata": {},
   "source": [
    "## Ignoring Broken Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-vision",
   "metadata": {},
   "source": [
    "__python has the following decorators to skip__:\n",
    "- `expectedFailure()`\n",
    "    - simply tess test runner to ignore this test if it fails\n",
    "- `skip(reason)`\n",
    "    - does not even run the test, requires string saying why test failed\n",
    "- `skipIf(condition, reason)`\n",
    "    - uses a basic comparision operators shuch as `==`\n",
    "- `skipUnless(condition, reason)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welsh-display",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: C:\\Users\\Vicktree\\AppData\\Roaming\\jupyter\\runtime\\kernel-63948ab3-2fef-45f6-964b-3dd7e7c54dca (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute 'C:\\Users\\Vicktree\\AppData\\Roaming\\jupyter\\runtime\\kernel-63948ab3-2fef-45f6-964b-3dd7e7c54dca'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vicktree\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import sys\n",
    "\n",
    "class SkipTests(unittest.TestCase):\n",
    "    @unittest.expectedFailure\n",
    "    def test_fails(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "    @unittest.skip(\"Test is useless\")\n",
    "    def test_skip(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "    @unittest.skipIf(sys.version_info.minor == 4, \"broken on 3.4\")\n",
    "    def test_skipif(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "    @unittest.skipUnless(\n",
    "    sys.platform.startswith(\"linux\"), \"broken unless on linux\"\n",
    "    )\n",
    "    def test_skipunless(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-representative",
   "metadata": {},
   "source": [
    "## Testing with Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-contrary",
   "metadata": {},
   "source": [
    "- `unittest` requires alot of boilerplate code and is an example of overusing OOP\n",
    "- `Pytest` does not require test cases to be classes\n",
    "- it takes advantage of the fact that python functions are objects and allows any properly named function to behave like a test\n",
    "- instead of providing a bunch of custom methods for asserting equality, it uses the `assert` statement to verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-defeat",
   "metadata": {},
   "source": [
    "- when we run pytest, it starts in the current folder and searches for any modules or subpackages with names beginning with the character `test_`\n",
    "- if any functions in the module also start with the `test` they will be executed\n",
    "- if there area any classes in the module whoses name starts with `Test` any methods on that class that start with `test_` will also be executed\n",
    "- pytest suppresses output from `print` statements if the test is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "flexible-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how simple a test is to write\n",
    "\n",
    "def test_int_float():\n",
    "    assert 1 == 1.0\n",
    "\n",
    "# we could also use a class\n",
    "\n",
    "class TestNumbers:\n",
    "    def test_int_float(self):\n",
    "        assert 1 == 1.0\n",
    "        \n",
    "    def test_int_str(self):\n",
    "        assert 1 == \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-piece",
   "metadata": {},
   "source": [
    "## One way to do setup and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-significance",
   "metadata": {},
   "source": [
    "- pytest provides more flexability for the `setup` and `teardown` methods\n",
    "- we can use `setup_method` and `teardown_method`\n",
    "- the one difference between the pytest startup/teardown is that both methods accept an argument: the function object representing the method being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-immunology",
   "metadata": {},
   "source": [
    "- we have additional `setup_class` and `teardown_class` methods are expected to be class methods and they accept a single argument representing the class in question\n",
    "- these methods only run when the class is initaited rather than on each test run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-district",
   "metadata": {},
   "source": [
    "- we also have the `setup_module` and `teardown_module` functions, which are run immediately before and after all tests (in functions or classes) in that module\n",
    "- these are useful for `one time` setup, such as creating a socket or database connection that will be used by all tests in the module\n",
    "- be careful with this one as it can introudce dependencies between tests if the object stores state that is not cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "banned-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_module(module):\n",
    "    print(f'setting up MODULE {module.__name__}')\n",
    "    \n",
    "    \n",
    "def teardown_module(module):\n",
    "    print(f'tearing down Modulle {module.__name__}')\n",
    "    \n",
    "    \n",
    "class BaseTest:\n",
    "    def setup_class(cls):\n",
    "        print(\"setting up CLASS {cls.__name__}\")\n",
    "    \n",
    "    def teardown_class(cls):\n",
    "        print(f'tearing down CLASS {cls.__name__}')\n",
    "        \n",
    "    def setup_method(self, method):\n",
    "        print(f'tearing down METHOD {method.__name__}')\n",
    "\n",
    "class TestCase1(BaseTest):\n",
    "    def test_method_1(self):\n",
    "        print(f'Running METHOD 1-1')\n",
    "    \n",
    "    def test_method_2(self):\n",
    "        print(f'RUNNING METHOD 1-2')\n",
    "    \n",
    "    \n",
    "class TestClass2(BaseTest):\n",
    "    def test_method_1(self):\n",
    "        print(f'RUNNING METHOD 2-1')\n",
    "    \n",
    "    def test_method_2(self):\n",
    "        print(f'RUNNING METHOD 2-2')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-agency",
   "metadata": {},
   "source": [
    "- the purpose of `BaseTest` class is to extract four methods that are otherwise identical to the test classes, and use inheritance to reduce the amount of duplicate code\n",
    "- from the point of view of `pytest` the two subclasses have not only two test methods each, but also have two setup and two teardown methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-voltage",
   "metadata": {},
   "source": [
    "## A completely different way to seup up variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-evening",
   "metadata": {},
   "source": [
    "- one of the most common uses for the various setup and teardown functions is to ensure certain class or module variables are available with a known value before each test method is run\n",
    "- pytest offers a completely different way of doing this, using what is known as `fixtures`\n",
    "- fixtures are basically named variables that are predefined in a test configuration file\n",
    "- this allows us to seprate configuration from the execution of tests, and allows fixtures to be used across multiple classes and modules\n",
    "- to use them, we add parameters to our test function\n",
    "- the names of the parametes are used to look up specific arguments in specially named functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extreme-courage",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-691cc52be896>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStatsList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mpytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvalid_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytest'"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from stats import StatsList\n",
    "\n",
    "@pytest.fixture\n",
    "def valid_stats():\n",
    "    return StatsList([1, 2, 2, 3, 3, 4])\n",
    "\n",
    "def test_mean(valid_stats):\n",
    "    assert valid_stats.mean() == 2.5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-operator",
   "metadata": {},
   "source": [
    "- each of the three test methods accepts a parameter named `valid_stats`\n",
    "- the parameter is created by calling the `valid_stats` function which was decorated with `@pytest.fixture`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-rochester",
   "metadata": {},
   "source": [
    "- fixtures can do alot more than return basic variables\n",
    "- a `request` object can be passed into the fixture factory provide useful infomation\n",
    "- `module`, `cls` and `function` attribute allow us to see exactly which test is requesting the fixture\n",
    "- the `config` attribute allows us to check command-line arguments and a great deal of other configuration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-bookmark",
   "metadata": {},
   "source": [
    "- the fixture creates a new empty temporary directory for files to be created in\n",
    "- it yeilds this for use in the test, but removes that directry using `shutil.rmtree` which recursively removes a directory and anything inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "federal-south",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-b29b2be3385e>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-b29b2be3385e>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    yeild dir\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import tempfile\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "@pytest.fixture\n",
    "def temp_dir(request):\n",
    "    dir = tempfile.mkdtemp()\n",
    "    print(dir)\n",
    "    yeild dir\n",
    "    shutil.rmtree(dir)\n",
    "\n",
    "def test_osfiles(temp_dir):\n",
    "    os.mkdir(os.path.join(temp_dir, \"a\"))\n",
    "    dir_contents = os.listdir(temp_dir)\n",
    "    assert len(dir_contents) == 2\n",
    "    assert \"a\" in dir_contents\n",
    "    assert \"b\" in dir_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-fundamental",
   "metadata": {},
   "source": [
    "- we can pass a `scope` parameter to create a fixture that lasts longer than one test\n",
    "- this is useful when setting up an expensive operation that can be reused multiple tests\n",
    "- temember that the resource reuse does not break the unit nature of tests\n",
    "- the scope can be one of the string `class`, `module`, `package` or `session`\n",
    "- it determines how long the arguments will be cached\n",
    "- the `session` caches it for the duration of the `pytest` run but the `module` only caches it for test in that module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture(scope=\"session\")\n",
    "def echoserver():\n",
    "    print(\"loading server\")\n",
    "    p = subprocess.Popen([\"python3\", \"echo_server.py\"])\n",
    "    time.sleep(1)\n",
    "    yield p\n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-dubai",
   "metadata": {},
   "source": [
    "## Skipping tests with Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-parliament",
   "metadata": {},
   "source": [
    "- `pytest.skip` function can skip a test\n",
    "- it has a single argument, which is a string describing why it was skipped\n",
    "- if called insite the function, the function is skipped\n",
    "- if called on the module level, all the tests in that module will be skipped\n",
    "- if we call it inside a fixture, all tests that call that funcarg will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confused-handbook",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-362c9ccc587a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_simple_skip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"fakeos\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytest'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pytest\n",
    "\n",
    "def test_simple_skip():\n",
    "    if sys.platform != \"fakeos\":\n",
    "        pytest.skip(\"Test works only on fakeOS\")\n",
    "    fakeos.do_something_fake()\n",
    "    assert fakeos.did_not_happen\n",
    "    \n",
    "# mark.skipif behaves simmilar to expectedFailure()\n",
    "# if xfail is not supplied, it will be expected to fail under all situations\n",
    "@pytest.mark.skipif(\"sys.version_info <= (3.0)\")\n",
    "    def test_python3():\n",
    "        assert b\"hello\".decode() == \"hello\"\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-replacement",
   "metadata": {},
   "source": [
    "## Imitating Expensive Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-lebanon",
   "metadata": {},
   "source": [
    "- imagine we need to use an `API` \n",
    "- we can use `Mock()` objects in out test to replace the roublesome methods with an object we can introspect\n",
    "- we create a `Mock` object for the `set` method and make sure that it is never called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cubic-armor",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-a39b5d75f8e3>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-a39b5d75f8e3>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    assert tracker.redis.set.call_count  = 0\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from flight_status_redis import FlightStatusTracker\n",
    "from unittest.mock import Mock\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def tracker():\n",
    "    return FlightStatusTracker()\n",
    "\n",
    "def test_mock_method(tracker):\n",
    "    tracker.redis.set = Mock()\n",
    "    \n",
    "    with pytest.raises(ValueError) as ex:\n",
    "        tracker.change_status(\"AC101\", \"lost\")\n",
    "    assert ex.value.args[0] == \"Lost is not a valid status\"\n",
    "    assert tracker.redis.set.call_count  = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-vegetation",
   "metadata": {},
   "source": [
    "- temporarily setting a library function to a specifc value is one of the few valid use cases for monkey-patching\n",
    "- the mock library provides a patch context manager  that allows us to replace attributes on existing libraries with mock objects\n",
    "- when the context manager exits, the orginal attribute is automatically restored so as not to impact other test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from unittest.mock import patch\n",
    "\n",
    "def test_patch(tracker):\n",
    "    tracker.redis.set = Mock()\n",
    "    fake_now = datetime.datetime(2015, 4, 1)\n",
    "    with patch(\"datetime.datetime\") as dt:\n",
    "        dt.now.return_value = fake_now\n",
    "        tracker.change_status(\"AC102\", \"on time\")\n",
    "    dt.now.assert_called_once_with()\n",
    "    tracker.redis.set.assert_called_once_with(\n",
    "        \"flightno:AC102\", \"2015-04-01T00:00:00|ON TIME\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-fiber",
   "metadata": {},
   "source": [
    "- we first construct a value called `fake_now`, which we set as the return value of the `datetime.dsatetime.now` function\n",
    "- we have to construct the object before we patch `datetime.datetime`, because otherwise we'd be calling the patched now function before we constructed it\n",
    "- the `with` statement invites the patch to replace the `datetime.datetime` module with a mock object, which is returned as `dt` value\n",
    "- the neat thing about a mock object is that you anytime you access an attribute or method on that object, it return another mock object\n",
    "- this when we accessed `dt.now`, it gives us a new mock object\n",
    "- we set the `return_Value` of that object to our `fake_now` object\n",
    "- when ever the `datetime.datetime.now` function is called, it will return our object instead of a new mock object\n",
    "- when the context manager is exited, the orginal `datetime.datetime.now()` functionality is restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-electronics",
   "metadata": {},
   "source": [
    "- if we find ourselves mocking out multiple elements in a given unit test, we should rething it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-announcement",
   "metadata": {},
   "source": [
    "## How much testing is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-notice",
   "metadata": {},
   "source": [
    "- how much of our code is actually being tested is easy to verify, we can just use the `coverage` coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-compression",
   "metadata": {},
   "source": [
    "- `coverage run coverage_unittest.py`\n",
    "    - generates a `.coverage` file\n",
    "- `coverage report`\n",
    "    - shows the coverage\n",
    "- `coverage html` \n",
    "    - shows us the coverage in the html with more info\n",
    "- we can use the coverage module with `pytest` as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ram Model of Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RAM Model__:\n",
    "- Each simple operation $ (_, *, -, =, if, call) $  takes exactly one time step\n",
    "\n",
    "- Loops and subroutines are not considered simple operation. They are the composition of many single-step operation\n",
    "\n",
    "- Each memory acess takes exactly one time step. Secondly we have as much memory as we need\n",
    "\n",
    "- Note this algorithm is not perfect as multiplication takes more time then addition, howerver its as close to exelent as we can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best, Worst and Average Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/01.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Worst-Case Complexity__:\n",
    "> the worst-case complexity of the algoithm is the function defined by the maximum number of steps taken in any instance of size $ n $. This represents the curve passing through the highest point in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Best-Case Complexity__:\n",
    "> The best-case complexity of the algorithm is the function defined by the minimum number of steps taken in any instance of size $ n $. This represents the curve passing through the lowest point of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Average-Case Complexity__:\n",
    "> The average-case complexity of the algorithm which is the function defined by the average number of steps over all instances of size n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Oh Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Big-Oh notation ignores the difference between multiplicative constanct. the function $ f(n) = 2n $ and $ g(n) = n $ are identical in Big Oh analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose an algorithm ran twice as fast in $ C $ then it did in $ Python $. The $ 2 $ is not important because it does not scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ f(n) = O(g(n)) $ means $ c * g(n) $ is an _upper bound_ on $ f(n) $. Thus there exists some constant $ c $ such that $ f(n) $ is always $ <= c*g(n) $ for larger enough $ n $ (i.e, $ n >= n_0 $ for some constant $ n_0 $ \n",
    "- $ f(n) = Ω(g(n))$ means $ c * g(n) $ is a _lower bound_ on $ f(n) $. Thus there exists some constant $ c $ such that $ f(n) $ is always $ >= c*g(n) $ for all $ n >= n_0 $\n",
    "- $ f(n) = Θ(g(n)) $ means $ c_1 * g(n) $ is an upper bound on $ f(n) $ and $ c_2 * g(n) $ is a lower bound on $ f(n) $, for all $ n >= n_0$. Thus there exist constants $ c_1 $ and $ c_2 $ such that $ f(n) <= c_1 * g(n) $ and $ f(n) >= c_2 * g(n) $. This means that $ g(n) $ provides a nice, tight bound on $ f(n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big O notation and worst-case analysis are tools that greatly simplify out ability to compare the effciency of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/03.png\" width=700></center>\n",
    "\n",
    "__Figure 2.3:__ Illustrating the big (a) $ O $, (b) $ Ω $ and (c) $ Θ $ notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growth Rates and Dominance Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/04.png\" width=600></center>\n",
    "\n",
    "__Figure 2.4:__ Growth rates of common functions measured in nanoseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaways from Table:\n",
    "- All such algorithms take roughly the same time for $ n = 10 $\n",
    "- Any algorithm with $ n! $ running times becomes usless for $ n ≥ 20 $\n",
    "- Algorithms whos running time is $ 2^n $ have a greater operating range, but become impractical for $ n > 40 $\n",
    "- Quadratic-time algorithms whos running time is $ n^2 $ remains usable to about $ n = 10,000 $ but quickly deteriorate with larger inputs. They are likely to be hopeless for $ n > 1,000,000 $\n",
    "- Linear-time and $ n lg n $ algorithms remain pratical on imputs of one billion items\n",
    "- An $ O(lg n) $ algorithm hardly breaks a serat for any imaginable value for $ n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominance Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many functions, the highest power will dominate. This is simmilar to taking the limit of something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Constant Functions__:\n",
    "> $ f(n) = 1 $: Such functions might measure the cost of adding two numbers, printing out a string or the growth realized by functions such as $ f(n) = min(n, 100) $. In the big picture there is no dependence on the parameter $ n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Log Functions__:\n",
    "> $ f(n) = log(n)$: Log time-complexity shows up in algorithms such as binary search. Such functions grow quite slowly as $ n $ gets big, but faster then the constant function (which is standing still, after all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear Functions__:\n",
    "> $ f(n) = n$: Such functions mesure the cost of looking at each item onece (or twie, or ten times) in an $ n$-element array, say to identify the biggest item, the smallest item, or compute the average value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Superlinear Functions:__\n",
    "> $ f(n) = n lg n $: This important functions arises in such algorithms as Quicksort and mergesort. They grow just a little faster than linear, just enough to be a different dominance class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quadratic Functions__:\n",
    "> $ f(n) = n^ 2 $: such functions measure the cost of looking at most or all _pairs_ of tiems in an $n$-element universe. This arises in algorithms such as insertion sort and selection sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cubic Functions__:\n",
    "> $ f(n) = n^3 $: Such functions enumerate through all _tiples_ of items in an $n$-element universe. there also arise in certain dynamic programming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exponential Functions__:\n",
    "> $ f(n) = c^n $ for a given constant $ c > 1 $: Functions like $ 2^n $ arise when enumerating all subsets of $ n $ items. As we have seen, exponential algorithms become useless fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Factorial Function__:\n",
    "> $ f(n) = n! $: Functions like $ n! $ arise when generating all permutations or ordering $ n $ items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ n! > 2^n > n^3 > n^2 > n log(n) > n > log(n) > 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Big-O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of two functions is governed by the dominant one, namely:\n",
    "$$ O(f(n)) + O(g(n)) → O(max(f(n), g(n))) $$\n",
    "$$ Ω(f(n)) + Ω(g(n)) → Ω(max(f(n), g(n))) $$\n",
    "$$ Θ(f(n)) + Θ(g(n)) → Θ(max(f(n), g(n))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expressions tell us that it is useful to simplify terms because everything else does not matter except the dominant terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(n) = O (n^2) $$\n",
    "$$ g(n) = o(n^2) $$\n",
    "$$ f(n) + g(n) = O (n^2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication is repeated addition. Consider multiplication by any constant c > 0, be it 1 or 1 million. Multiplying a function by a constant can not affect its asymptotic behavioir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ O(c* f(n)) → O(f(n)) $$\n",
    "$$ Ω(c* f(n)) → Ω(f(n)) $$\n",
    "$$ Θ(c* f(n)) → Θ(f(n)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, when two functions in a product are increasing, both are important. The function $ O (n! logn)$ dominates $ n! $ just as much as $ log n $ dominates 1\n",
    "\n",
    "$$ O(f(n)) * O(g(n)) → O(f(n) * g(n)) $$\n",
    "$$ Ω(f(n)) * Ω(g(n)) → O(f(n) * g(n)) $$\n",
    "$$ Θ(f(n)) * Θ(g(n)) → Θ(f(n) * g(n)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O relationships are transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ f(n) = O(g(n)) $ and $ g(n) = O(h(n))$, the $ f(n) = O(h(n)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning About Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will analyze the selection sort algorithm, which repeatedly identifies the smallest remaining unsorted element and puts at the end of the sorted portion of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/05.png\" width=200></center>\n",
    "\n",
    "__Figure 2.5__: Animation of selection sort in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    selection_sort(int s[], int n)\n",
    "    {\n",
    "        int i,j; /* counters */\n",
    "        int min; /* index of minimum */\n",
    "            for (i=0; i<n; i++) {\n",
    "                min=i;\n",
    "                for (j=i+1; j<n; j++)\n",
    "                    if (s[j] < s[min]) min=j;\n",
    "                swap(&s[i],&s[min]);\n",
    "            }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer loop goes around $ n $ times. The nested inner loop goes aroung $ n - i - 1 $ times, where $ i $ is the index of the outer loop. The exact number of times the $ if $ statement is executed is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/06.png\" width=300></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the sum is doing is adding up the integers in decreasing order starting from $ n - 1$ i.e\n",
    "\n",
    "$$ S(n) = (n - 1) + (n -2) + (n - 3) + ... + 2 + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with the Big-O, we are only intersted in the _order_ of the expression. One way to think about it is that we are adding up $ n - 1 $ temrs, whose average value is about $ n / 2 $. This yeilds \n",
    "$$ S(n) = n(n-1)/2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about it is in terms of upper and lower bounds. We have $ n $ terms at most, each of which is at most $ n - 1 $. Thus $ S(n) <= n(n -1) = O(n^2)$. We have $ n/2 $ temrs each that are bigger then $ n /2 $. Thus $ S(n) => (n/2) * (n/2) = Ω(n^2) $. This tells us that the running time is $ Θ(n^2)$, meaning that selection sort is quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic rule of thumb in Big-O analysis is that worst-case running time follows from multiplying the largest number of times each nested loop can iteratie. Considet the insertion sort algorithm whos inner loops are shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for (i=1; i<n; i++) {\n",
    "         j=i;\n",
    "         while ((j>0) && (s[j] < s[j-1])) {\n",
    "             swap(&s[j],&s[j-1]);\n",
    "             j = j-1;\n",
    "         }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often does the inner while loop iterate? Its trickey because there are different stopping conditions: one to prevent us from running off the bounds of the array $ (j > 0)$ and the other ot mark when the element finds its propper place in sorted order $ (s[j] < s[j-1])$. Since worst-case analysis seeks an upper bound on the running time, we ignore the early termination and assume that this loop always goes around $ i $ times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infact we can assume it always goes around $ n $ times since $ i < n $. Since the outer loop goes around $ n $ times, insertion sort must be a quadratic-time algorithm $ O (n^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sting Pattern Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern matching is the most fundamental algorithmic operation on text strings. This algorithm implements the find command available in any web browser or text editor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__: Substring Pattern Matching\n",
    "\n",
    "__Input__: A string $ t $ and $ a $ apttern string $ p $\n",
    "\n",
    "__output__: Does $ t $ contain the pattern $ p $ as a substring, and if so where?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/07.png\" width=200></center>\n",
    "\n",
    "__Figure 2.6__: Searching for the substring _abba_ in the text _aababba_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    int findmatch(char *p, char *t)\n",
    "    {\n",
    "        int i,j; /* counters */\n",
    "        int m, n; /* string lengths */\n",
    "        m = strlen(p);\n",
    "        n = strlen(t);\n",
    "        for (i=0; i<=(n-m); i=i+1) {\n",
    "            j=0;\n",
    "            while ((j<m) && (t[i+j]==p[j]))\n",
    "                j = j+1;\n",
    "            if (j == m) return(i);\n",
    "        }\n",
    "        return(-1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner while loop goes around at most $ m $ times and potentially for less when the patern match fails. This plus two other statements, lie within the outter $ for $ loop. The outer loop goes around at most $ n - m $ times. Since no complete alignment is possible once we get too far to the right of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time complexity of nested loops multiplie, so this gives a worst-case running time of $ O ((n - m)(m + 2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have the problem where we do not know the runtime of `strlen`. SO we must guess, and we update out complexity $ O(n + m + (n-m)(m + 2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply out the complexity and reduce it, we get $ O (n + m + nm - m^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make multiple other observations, such as knowing that $ n > m $ because its impossible to have $ p $ as a substring of $ t $ for any pattern lower than the text itsef, we can get the final big-o to $ O(n +nm - m^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, observe that the $ -m^2 $ temr is negative and thus only serves to lowet the value. Since Big-O gives an upper bound, we can drop any negative terms without invalidating the upper rbound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally, we are left with $ O(nm) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested summations often arise in the analysis of algorithms with nested loops. Consider the problem of matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem__: Matrix Multiplication\n",
    "\n",
    "__Input__: Two matrices $ A $ (of dimension $ x * y $) and $ B $ (dimension $(y * z)$ \n",
    "\n",
    "__Output__: An $ x * y $ matrix $ C $ where $ C[i][j] $ is the dot product of the $ ith $ row of $ A $ and the $ jth $ column of $ B $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elemntary algo for matrix multiplication is implemented as a tight product of three nested loops:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for (i=1; i<=x; i++)\n",
    "        for (j=1; j<=y; j++) {\n",
    "            C[i][j] = 0;\n",
    "            for (k=1; k<=z; k++)\n",
    "                C[i][j] += A[i][k] * B[k][j];\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of multiplications $ M(x, y, z) $ is given by the following summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/08.png\" width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sums get evaluated from the right inward. The sum of $ z $ is ones is $ z $ so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/09.png\" width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of $ y $ $ zs $is just as simple, $ yz $ so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/10.png\" width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the sum of $ x $ $ yzs $ is $ xyz $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the running of this matrix multiplication algorithm $ O(xyz) $. If we consider the common case where all three dimensions are the same, this becomes $ O (n^3)$, i.e.., a cubic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logarithms and Their Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logarithm is simply an inverse exponential function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saying $ b^x = y $ is the equvalent to saying that $ x = log_b(y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, this definition is the same as saying  $ b^{log_b(y)} = y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs and Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary search is a good example of an $ O(log n) $ algorithm. The number of steps the algorithm take equals the number of times we can halve $ n $ untill only one name is left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bt definition, this is exactly $ log_2(n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithms and Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/11.png\" width=200></center>\n",
    "\n",
    "__Figure 2.7__: A height $ h $ tree with $ d $ children per node as $ d^h $ leaves. Here $ h = 2 $ and $ d = 3 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary tree of height $ 1 $ can have up to $ 2 $ leaf nodes, while a tree of height $ 2 $ can have up to four leaves. What is the height of $ h $ of a rooted binary tree with $ n $  leaf nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of leaves doubles every time we increase the height by one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for $ n $ leaves, $ n = 2^h $ which implies that $ h = log_2(n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we generalize to trees that have $ d $ children, where $ d= 2 $ for the case of binary trees? A tree of height $ 1 $ can have up to $ d $ leaf nodes, while one of height $ 2 $ can have up to $ d^2 $ leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of possible eleaves multiplies by $ d $ every time we increase the height by one, so to account for $ n $ leaves, $ n = d^h $, which implies that $ h = log_d(n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway is that short trees can have very many leaves and thus is the main reason binary trees prove fundamental to the design of fast data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithms and Bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are two bit patterns of lenght $ 1 (01) $ and four of lenght $ 2 (00, 01, 10, 11) $. How many bits $ w $ do we need to represent any of one of $ n $ different possibilities, be it one of $ n $ items or the integers from $ 1 $ to $ n $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key observation is that there mist be at least $ n $ different bit patters of lenght $ w $. Since the number of different bit patterns doubles as you add each bit, we need at least $ w $ bits where $ 2^w = n $, i.e., we need $ w = log_2(n) $ bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithms and Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithms are useful for multiplication, particularly for exponentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log_a(xy) = log_a(x) + log_a(y)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the log of a product is the sum of logs property is a direct consequence of the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log_a(n^b) = b*log_a(n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we compute $ a^b $ for any $ a $ and $ b $ using the $ exp(x) $ and $ ln(x) $ functions on your calculator, where $ exp(x) = e^x $ and $ ln(x) = log_e(x) $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know $ a^b = exp(ln(a^b)) = exp(bln(a)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the problem is reduced to one multiplication plus one call to each of these functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Exponentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we wanted to compute exact value for $ a^n $ for some large $ n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the simplest algorithm performs $ n - 1 $ multiplications by computing $ a * a * ... a $. But we can do better by noting that $ n = [n /2] + [n/2] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ n $ is even, then $ a^n = (a^{n/2})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ n $ is odd, then $ a^n = a(a^{[n/2]})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In either case, we have halved the size of our exponent at the cost of, at most, two multiplications so $ (lg n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    function power(a, n)\n",
    "        if (n = 0) return(1)\n",
    "        x = power(a, n/2 )\n",
    "        if (n is even) then return(x2)\n",
    "            else return(a × x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithms and Summations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Harmonic number_ arise as a special case of arithmetic progression, namely $ H(n) = S(n, -1) $. They reflect the sum of the progression of simple reciprocals, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/12.png\" width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows up in places such as the federal sentencing guidelines for fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The harmonic number is important because they usually explain \"where the log comes from\" when one magically pops out from algebraic manipulation. The average complexity of quicksort is the sumation <img src=\"images/chap2/13.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b^x = y $$ \n",
    "$$ x = log_b(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important Bases__:\n",
    "- Binary Logarithm\n",
    "    - Base $b = 2$; we have seen how the base arises whenever repeated halving (binary search) or doubling (nodes in tree) occur. Most algorithmic applications of logarithms imply binary logarithms\n",
    "- Natural Log\n",
    "    - Base $b = e$; usually denoted $ ln(x) $, is base $ e = 2.71828... $ logarithm. The inverse of $ ln(x) $ is the exponential function $ exp(x) = e^x $ on your calculator\n",
    "- Common Logarithm\n",
    "    Base $ n = 10 $; less common today is the base-10, usually denoted as $ log(x)$. The base was employed in slide rules and algorithm books in days before pockey calculators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log_a(xy) = log_a(x) + log_a(y) $$\n",
    "$$ log_a(b) = log_c(b)/log_c(a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Key Ideas__:\n",
    "- The base of the logarithm has no real impact on the growth rate. Compate the following three values; $ log_2  (1,000,000) = 19.93) $, $ log_3 (1,000,000) = 12.575 $ and $ log_100 (1,000,000) = 3 $.A big change in the base of the log produces little difference in the value of the log. The conversion factor is lost to Big-O notation whenever $ a $ or $ c $ are constants\n",
    "- Logarithms cut any function down to size: the growth rates of the logatihm of any polynomial function is O (lg n): this is because $ log_a(n^b) = b*log_a(n) $. The power of binary search on wide range of problems is a consequence of this observation. Note that doing a binary search on a sorted array of $ n^2 $ things requires only twice as many comparisons as a binary search on a $ n $ things. Same with factorals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/chap2/14.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ n! > c^n > n^2 > n^2 > n^1+e > n log n > n > sqrt(n) > log^2(n) > log(n) > log(n)/log log(n) > log log (n) > a(n) > 1 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows that clever sorting algorithms exist that run in $ O(n log n)$ and naive algorithms run in $ O(n^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses of Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binary search tests whether an item is in a dictionary in $ O(log n) $ time, provided the keys are all sorted. Preprocessing is perhpas the single most important application of sorting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closest Pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ginven a set of $ n $ numbers, how do we find the pair of numbers that have the smallest difference between them? Once the numbers are sorted, the closest pair of numbers must lie next to each other somewhere in sorted order. Thus, a linear-time scan through them completes the job, for a total of $ O (n log n) $ time including the sorting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element Uniqueness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any duplicates in a given set of $ n $ items? This is a special case of the closest-pair problem above, where we ask if there is a pair separated by a gap of zero. The most efficient algortihm sorts the numbers and then does a linear scan through checking all adjacent pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a set of $ n $ items, which element occurs the largest number of times in the set? If the items are sorted, we can sweep from left to right and count them, since all identical items will be lumped together during sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To find out how often an arbitrary element $ k $ occurs, look up $ k $ using binary search in a sorted array of keys. By walking to the right, we can count in $ O(log n + c) $ time, where c is the number of occurences of $ k $. Even better, the number of instances of $ k $ can be found in $ O (log n ) $ time by using binary search to look for the position of both $ K - dx $ and $ k + dx $ where dx is arbitrarily small, and then taking the difference of these positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the $ kth $ largest item in an array? If the keys are placed in sorted order, the $ kth $ largest can be found in constant time by simply looking at $ kth $ position of the array. In particular, the median element appears in the $ (n/2)$ position in sorted order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conver Hulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/02.png)\n",
    "\n",
    "__Figure 1__: The convex hull of a set of points (1), constructed by left-to-right insertion. Think of it like a rubber band around toothpicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the polyogn of smallest area that contains a given set of $ n $ points in two dimensions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the points sorted by $x$-coordinate, the points can be inserted from left to right into the hull. Since the right-most point is always the boundary, we know that will appear in the hull. Adding this new right-most point may cause others to be deleted, but we can quickly identify these points because they lie inside the polygon formed by adding the new points. These points will be neighbors of the previous point we inserted, so they will be easy to find and delete. The total time is linear after the sorting has been done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, never be afraid to spend time sorting, provided you use an efficient sorting routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given an efficent algorithm to determine whether two sets (of size $ m $ and $ n $, respectively) are disjoint. Analyze the worst-case complexity in terms of $ m $ and $ n $ considering the case where $ m $ is substantially smaller than $ n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_First sort the big set_ : the big set can be sorted in $ O (n log n) $ times. We can now do a binary search with each of the $ m $ elements in the second, looking to see if it exists in the big set. The total time will be $ O ((n + m ) log n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_first sort the small set_ : The small set can be sorted in $ O (m log m) $ time. We can now do a binary search with each of the $ n $ elements in the big set, looking ot see if it exists in the small one. The total time will be $ O ((n + m) log m) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sort both sets_ : Observe that once the two sets are sorted, we no longer have to do binary search to detect a common element. W can just compare the smallest of the two sorted sets, and discard the smaller one if they are not identical. By repeating this idea recursively on the now smaller sets, we can test for duplication in linear time after sorting. The total cost is $ O (n log n + m log m + n + m )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the small-set sorting tumps bit-set sorting since $ log m < log n $ when $ m < n $. Thus sorting the small set is the best of these options. Note that this is linear when $ m $ is constant in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore that expected linear time can be avhived by hashing. Build a hash table containing the elements of both sets, and verify that collisions in the same bucket are in fact identical elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pragmatic of Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What order do we want to sort items in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Increasing__ or __Decreasing__:\n",
    "    - A set of keys $ S $ are sorted in ascending order when $ S_i <= S_{i+1} $ for all $ 1 <= i <= n $. They are in descending order when $ S_i >= S_{i+1} $ for all $ 1 <= i < n $. Different applications call for different order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort __key__ or entire __record__:\n",
    "    - Sorting a data set involved maintaining the integrity of complex data records. A mailing list of names, adresses, and phone numbers may be sorted by name as the key field, but it had better retain the linkage between names and adresses. Thus we need to specify which feild is the key feild in our complex record and understand the full extent of each record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What should we do with equal keys: \n",
    "    - Sometimes the relative order of the key matter. What if two names are the same, Micheal Jordan. Well you might want to sort according to popularity and it matters which one comes first. It might be required to leave the items in the same relative order as in the orginal permutation. Sorting algorithms that automatically enforce this requirement are called _stable_. Fast algortihms are generally not stable. __Stability can be achived for any sorting algortihm by adding the inital position as a secondary key__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What about __non-numerical__ data:\n",
    "    - Alphabetizing the sorting text strings. Libraries have very complete and complicated rules concerning the relative _collating sequence_ of characters and punctuations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heapsort: Fast Sorting via Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection sort repeatedly extracts the smallest remaining element from the unsorted part of the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SelectionSort(A)\n",
    "        For i = 1 to n do\n",
    "            Sort[i] = Find-Minimum from A\n",
    "            Delete-Minimum from A\n",
    "        Return(Sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection sort performs $ n $ iterations, where the average iteration takes $ n / 2 $ steps for a tital of $ O(n^2)$ time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes $ O (1) $ time to remove a particular item fom an unsorted array once it has been located, but $ O(n) $ time to find the smallest item. This is essentially what a priority queue does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we replace the data structute with a better priority queue implementation, either a heap or a balanced binary tree. Operations within the loop now take $ O (log n) $ time each instead of $ O (n) $. Using such a priority queue implementation speeds up selection sort from $ O(n^2) $ to $ O(n log n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name typically given to this algorithm, _heapsort_ , obscures the relationsship betwen them, but heapsort is nothing but an implementation of selection sort using the right data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on Python Heaps:\n",
    "https://www.educative.io/edpresso/what-is-the-python-priority-queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heaps are a data structure for effciently supporting the priority queue operations insert and extract-min. They work by maintaining a partial order on the set of elements which is weaker than the sorted order (so it can be efficent to maintain) yet stronger than random order (so the minimum element can be quickly identified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power in any hierarchically-structured organization is reflected by a tree, where each node in the tree represents a person, and $ edge (x, y) $ implies that $ x $ directly supervises (or dominates) $ y $. The fellow at the root sits at the \"top of the heap\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, a _heap-labled tree_ is defined to be a binary tree such that the key labeling of each node _dominates_ the key labeling of each of its children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/03.png)\n",
    "\n",
    "__Figure 2__: A heap-labeled tree of important years American history $ (1) $, with the corresponding implicit heap representation $(r) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a min-heap, a node dominates its children by containing a smaller key than they do, while in a max-heap, parent nodes dominate by being bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most natural implementation of this binary tree would store each key in a node with pointers to its two children. As with binary search trees, the memory used by the pointers can easily outweight the size of the keys, which is the data we are intrested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heap is a slick data structure that enables us to represent binary trees without using any pointers. We will store the root of the tree in the first position, respectively. In general, we will store $ 2^l $ keys of the $ lth $ level of a complete binary tree from left to right in positions $ 2^{l-1) $ to $ 2^l -1$ as shown in __Figure 2(r)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    typedef struct {\n",
    "        item_type q[PQ_SIZE+1]; /* body of queue */\n",
    "        int n; /* number of queue elements */\n",
    "    } priority_queue;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is espically nice about this representation is that the position of the parent and children of the key at position $ k $ are readily determined. The left child of $ k $ sits in position $ 2k $ and the right child in $ 2k + 1 $, while the parent of $ k $ holds court in position $ [n/2] $. This we can move around the tree without pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pq_parent(int n)\n",
    "    {\n",
    "        if (n == 1) return(-1);\n",
    "        else return((int) n/2); /* implicitly take floor(n/2) */\n",
    "    }\n",
    "    pq_young_child(int n)\n",
    "    {\n",
    "    return(2 * n);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what the catch with this, representing a binary tree without pointers? Well all internal nodes still take up space in our structure, since we must represent a full binary tree to maintain the positional mapping between parents and children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space efficency thus demands that we not allow holes in out tree, that each level be packed as much as it can be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the last level may be incomplte. By packing the elements of the last level as far to the left as possible, we can represent an $ n$-key tree using exactly $ n $ elements of the array. If we do not enforce this structural constraints, we might need an array of size $ 2^n $ to store the same elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all but the last level is always filled, the height $ h $ of an $ n $ element heap is logarithmic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss of flexiblity means we cannot use this idea to represent a binary search tree but works for heaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __We cannto efficiently search for a particular key in a heap. Binary search does not work because a heap is not a binary search tree__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Heaps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heaps can be constructed incrementally, by inserting each new element into the left-most open spot in the array; namely $ (n + 1 )st $ position of a previously $n$-element heap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures the desired balanced shape of the heap tree, but does not necessarily maintain the dominance ordering of the keys. The new key might be less than its parent in a min-heap, or greater than its parent in a max-heap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to swap any such dissatisfied element with its parent. The old parent is now happy, because it is properly dominated. The other child of the old parent is still happy, because it is now doiminated by an element more extreeme than its previous parent. The new element is now happier, but may still dominate its new parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now recur at a higher level, bubbling up the new key to its proper position in the hierarchy. Since we replace the root of the subtree by a larger one at each step, we pereserve the heap order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Heap Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pq_insert(priority_queue *q, item_type x)\n",
    "    {\n",
    "    if (q->n >= PQ_SIZE)\n",
    "        printf(\"Warning: priority queue overflow insert x=%d\\n\",x);\n",
    "    else {\n",
    "        q->n = (q->n) + 1;\n",
    "        q->q[ q->n ] = x;\n",
    "        bubble_up(q, q->n);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bubble_up(priority_queue *q, int p)\n",
    "    {\n",
    "        if (pq_parent(p) == -1) return; /* at root of heap, no parent */\n",
    "    \n",
    "    if (q->q[pq_parent(p)] > q->q[p]) {\n",
    "        pq_swap(q,p,pq_parent(p));\n",
    "        bubble_up(q, pq_parent(p));\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The swap process takes constant time at each level. Since the height of an $ n $ element heap is $ [lg n] $, each insertion takes at most $ O (log n) $ time. Thus an initial heap of $ n $ elements can be constructed in $ O (n log n) $ time though $ n $ usch isertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pq_init(priority_queue *q)\n",
    "    {\n",
    "        q->n = 0;\n",
    "    }\n",
    "\n",
    "    make_heap(priority_queue *q, item_type s[], int n)\n",
    "    {\n",
    "        int i; /* counter */\n",
    "        pq_init(q);\n",
    "        for (i=0; i<n; i++)\n",
    "            pq_insert(q, s[i]);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining operations are identifying and deleting the dominant element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identification is easy since the top of the heap sits in the first position of the array. Removing the top element leaves a hole in the array. This can be filled by moving the element in the $ right-most $ leaf (sitting in the $ nth $ position of the array) into the first position. The shape of the tree has been restored but the key of the root may no longer satisfy the heap propert and the relevent swaps need to be made. The dissatisfied element _bubles down_ the heap untill it dominates all its children, even perhaps by becoming a leaf node and ceasing to have any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _peroclate-down_ operation describes above is called __heapify__ because it merges two heaps (the subtrees below the orginal root) with a new key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    item_type extract_min(priority_queue *q)\n",
    "    {\n",
    "        int min = -1; /* minimum value */\n",
    "        \n",
    "        if (q->n <= 0) printf(\"Warning: empty priority queue.\\n\");\n",
    "        else {\n",
    "            min = q->q[1];\n",
    "\n",
    "            q->q[1] = q->q[ q->n ];\n",
    "            q->n = q->n - 1;\n",
    "            bubble_down(q,1);\n",
    "        }\n",
    "\n",
    "        return(min);\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    bubble_down(priority_queue *q, int p)\n",
    "    {\n",
    "        int c; /* child index */\n",
    "        int i; /* counter */\n",
    "        int min_index; /* index of lightest child */\n",
    "\n",
    "        c = pq_young_child(p);\n",
    "        min_index = p;\n",
    "\n",
    "        for (i=0; i<=1; i++)\n",
    "            if ((c+i) <= q->n) {\n",
    "                if (q->q[min_index] > q->q[c+i]) min_index = c+i;\n",
    "            }\n",
    "\n",
    "        if (min_index != p) {\n",
    "            pq_swap(q,p,min_index);\n",
    "            bubble_down(q, min_index);\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reach a leaf after $ [lg n] $ __bubble_down__ steps, each constrant time. Thus troot deletion is completed in $ O (log n) $ time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exchanging the maximum element with the last element and calling __heapify__ repeatedly gives an $ O (n log n) $ sorting algorithm, named __Heapsort__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    heapsort(item_type s[], int n)\n",
    "    {\n",
    "\n",
    "    int i; /* counters */\n",
    "    priority_queue q; /* heap for heapsort */\n",
    "\n",
    "    make_heap(&q,s,n);\n",
    "\n",
    "    for (i=0; i<n; i++)\n",
    "        s[i] = extract_min(&q);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Heapsort takes advantage of the heap datastructure which is an implementation of a priortiy queue. Since the top of the heap must be a min or max value, we can simply take the top value and begin moving it around. The purpose of heapsort is to efficiently sort an unsorted array. We do this via proxy tree. Simply take an array, transform it into a heap. This means that the largest value or smallest value is at the top, but not that the entire heap is sorted. So what we do is take the largest value and begin to move it down to the leaf node.  Now the largest item in the heap is located at the last node, which is great. We know that it is in its sorted position, so it can be removed from the heap completely. But theres still another step, making sure that the new root node elements in the correct place! It highly unlikely that the item that we swapped into the root node position is in the right location, so we'll move down the root node position is in the right location; this bubbling down, is called heapify. The algorithm repeats self untill we arrive at just a single node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heapsorts worst-case is $ (n log n) $ which is the best you can hope for in a sorting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Heap constuction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heaps can be constructed faster than $ O (n log n) $ time by using __bubble_down__ procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pack the $ n $ keys into for our heap into the first $ n $ elements of our priority-queue array. The shape of our heap will be correct but the dominance order will be messed up.  To restore it, we have to consider the last $ nth $ position. It represents leaf of the tree and so dominates its nonexistant children.  Same canse for the $ n/2 $ positions in the array, because all are leaves. If we continue to walk backwards, through the array we will finally encounter an internal node with children. The element may not dominate its children, but its children represent well-formed (if small) heaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    make_heap(priority_queue *q, item_type s[], int n)\n",
    "    {\n",
    "    int i; /* counter */\n",
    "    \n",
    "    q->n = n;\n",
    "    for (i=0; i<n; i++) q->q[i+1] = s[i];\n",
    "\n",
    "    for (i=q->n; i>=1; i--) bubble_down(q,i);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying the number of calls to __bubble_down__ $ (n) $ times an upper bound on the cost of each operation $ O(log n)) $ gives us a running time analysis of $ O (n log n )$. This would make it no faster than the incremental insertion algorithm described above, but note this is indeed an upper bound because insetion will take $ [lg n] $ steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bubble down__ takes time proportial to the height of the heaps it is merging. Most of these heps are extremely small. In a full tree of $ n $ nodes, there are $ n/2 $ nodes that are leaves, $ n/4 $ nodes that are height, $ n/8 $ nodes that are height $ 2 $ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are at most $ [n/2^{h+1}] $ nodes of hight $ h $, so the cost of builiding a heap is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series quickely converges to linear because the puny contribution of the numerator $ h $ is crushed by the $ 2^h $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the heap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Given an array based heap on $ n $ elements and a real number $ x $, effciently determine whether the $ kth $ smallest element in the heap is greater than or equal to $ x $. Your algorithm should be $ O (k) $ in the worst-case, independent of the size of the heap. You do not need to have to find the $ kth $ smallest elemen, but only determine its relationship to $ x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad Solutions:\n",
    "- Call exact-min $ k $ times, and test wheter all of these are less than $ x $. This explicitly sorts the first $ k $ elements and so gives us more information than the desired answers, but it takes $ O (k log n) $ time to do so\n",
    "- The $ kth $ smallest element cannot be deeper than the $ kth $ level of the heap since the path from the root must go rhough elements of decreasing value. Thus we can look at all the elements on the first $ k $ levels of the heap and count how many of them ar less than $ x $, stopping when we either find $ k $ of them or run out of elements. This is correct, but takes $ O (min(n, 2^k)) $ time, sicne the top $ k $ elements have $ 2^k $ elemtns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $ O(k) $ solution can look at only $ k $ elements smalelr than $ x $, plus at most $ O(k) $ elements greater than $ x $. COnsider the following procedure called at the root with $ i = 1 $ with $ count = k $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     int heap_compare(priority_queue *q, int i, int count, int x)\n",
    "    {\n",
    "    if ((count <= 0) || (i > q->n) return(count);\n",
    "\n",
    "    if (q->q[i] < x) {\n",
    "        count = heap_compare(q, pq_young_child(i), count-1, x);\n",
    "        count = heap_compare(q, pq_young_child(i)+1, count, x);\n",
    "        }\n",
    "        return(count);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure searches the children of all nodes of weight smaller than $ x $ untill either $ (a) $ we have found $ k $ of them, when it return $ 0 $, or $ (b) $ they are exhausted, when it returns a value greater than zero; thus it will find enough small elements if they exist. The onyl nodes whos children we will look at are those $ < x $ and at most $ k $ of these in total. Each have at most visited two children, so we visit at most $ 3k $ nodes, for a total of $ O(k) $ time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting By Incremental Insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an arbitrary element from the unsorted set, put it in the proper position in the sorted set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    InsertionSort(A)\n",
    "        A[0] = −∞\n",
    "        for i = 2 to n do\n",
    "            j = i\n",
    "            while (A[j] < A[j − 1]) do\n",
    "            swap(A[j],A[j − 1])\n",
    "            j = j − 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althoug insertion sort takes $ O(n^2) $ in the worst case, it performs considerably better if the data is almost sorted, since few iterations of the inner loop suffice to shift it into proper position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insertion sort is the simplest example of _incermental sort_ technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastest sorting algorithms based on incremental insetion follow from more efficient data structures. Insertion into a balanced search tree takes $ O (log n) $ per operation or a total of $ O (n log n) $ time to construct the tree. An in-order traversal reads through the elements in sorted order to complete the jon in linear time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MergeSort: Sorting By Divide and Conquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/05.png)\n",
    "\n",
    "__Figure 3__: Animation of MergeSort in Action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recursive approach to sorting involves partitioning the elements into two groups, sorting each of the smaller problems recursively and then interleaving the two sorted lists to totall order the elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mergesort(A[1,n])\n",
    "        Merge( MergeSort(A[1, [n/2]), MergeSort(A[n/2 + 1,n]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficency of mergesort depends on how we combine the two sorted halves into a single sorted list. We could add them to a list and do heapsort but that destroys all the work spent sorting the individual component lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead we _merge_ the two lists. Repeating the merge operation untill both lists are empty merges two sorted lists into one (with a total of $ n $ elements betwen them) into one, using at most $ n - 1$ comparisons or $ O(n) $ total work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume for simplicitly that $ n $ is a power of $ 2 $, the $ kth $ level consists of all $ 2^k $ calls to __mergesort__ processing subranges fo $ n/2^k $ elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work done on the $ (k = 0)$th level involves merging two sorted lists, each of size $ n/2$, for a total of at most $ n - 1$ comparisions. The work done on the $ (k = 1)$th level invovles merging two pairs of sorted lists, each of $ n/4 $, for a total of at most $ n - 2 $ comparisons. In general, the work done on the $ kth $ level invovled merging $ 2^k$ pairs sorted lists, each of size $ n/2^{k +1} $, for a total of at most $ n - 2^k $ comparisons. _Linear work is done merging all the elements on each level_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the expensive work with the most comparsions is at the top level. the number of elements in a subproblem gets halved at rach leve. This the number of times we can halve $ n $ untill we get $ 1 $ is $ [lg_2 n] $. Since the recursion goes $ lg n $ level deep, a linear amount of work is done per level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge sort takes $ O (n log n) $ time in the worst case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mergesort is a good algorithm for sorting linked lists, because it does not rely on random access to elements as does heapsort or quicksort. The disadvantage is the need for an auxilliary bufffer when sorting arrays. To merge two sorted arrays,  we need to use a third array to store the resuls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MergeSort Pesudo-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    mergesort(item_type s[], int low, int high)\n",
    "    {\n",
    "        int i; /* counter */\n",
    "        int middle; /* index of middle element */\n",
    "\n",
    "        if (low < high) {\n",
    "            middle = (low+high)/2;\n",
    "            mergesort(s,low,middle);\n",
    "            mergesort(s,middle+1,high);\n",
    "            merge(s, low, middle, high);\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MergeSort Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    merge(item_type s[], int low, int middle, int high)\n",
    "    {\n",
    "        int i; /* counter */\n",
    "        queue buffer1, buffer2; /* buffers to hold elements for merging */\n",
    "\n",
    "        init_queue(&buffer1);\n",
    "        init_queue(&buffer2);\n",
    "\n",
    "        for (i=low; i<=middle; i++) enqueue(&buffer1,s[i]);\n",
    "        for (i=middle+1; i<=high; i++) enqueue(&buffer2,s[i]);\n",
    "\n",
    "        i = low;\n",
    "        while (!(empty_queue(&buffer1) || empty_queue(&buffer2))) {\n",
    "        if (headq(&buffer1) <= headq(&buffer2))\n",
    "            s[i++] = dequeue(&buffer1);\n",
    "        else\n",
    "            s[i++] = dequeue(&buffer2);\n",
    "    }\n",
    "\n",
    "    while (!empty_queue(&buffer1)) s[i++] = dequeue(&buffer1);\n",
    "    while (!empty_queue(&buffer2)) s[i++] = dequeue(&buffer2);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.educative.io/edpresso/merge-sort-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuickSort: Sorting By Randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we select a random item $ p $ from the $ n $ items we seek to sort. Quicksort seprates the $ n - 1 $ other items into two piles: a low pile containing all the elements that appear before $ p $ in sorted order and a high pile containing all the elements that appear after $ p $ in sorted order. Low and high denote the array positions we place the respective piles, leaving a single slot between them for $ p $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/06.png)\n",
    "\n",
    "__Figure 4__: Animation of quicksort in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitioning does two things. First, the pivot element $ p $ ends up in exact array position it will reside in the final sorted order. _Thus we can now sort the elements to the left and the right of the pibot independently!_. This gives us a recursive sorting algorithm, since we can use the partitioning approach to sort each subproblem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quicksort Pesudo-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    quicksort(item_type s[], int l, int h)\n",
    "    {\n",
    "        int p; /* index of partition */\n",
    "\n",
    "        if ((h-l)>0) {\n",
    "            p = partition(s,l,h);\n",
    "            quicksort(s,l,p-1);\n",
    "            quicksort(s,p+1,h);\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can partition the array in one linear scan for a particular pivot element by maintaining three sections of the array: less than the pivot (to the left of `firsthight`), greater than or equal to the pivot (between `firsthight` and `i`) and, unexplored (to the right of `i`) as implemented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    int partition(item_type s[], int l, int h)\n",
    "    {\n",
    "        int i; /* counter */\n",
    "        int p; /* pivot element index */\n",
    "        int firsthigh; /* divider position for pivot element */\n",
    "\n",
    "        p = h;\n",
    "        firsthigh = l;\n",
    "        for (i=l; i<h; i++)\n",
    "            if (s[i] < s[p]) {\n",
    "                swap(&s[i],&s[firsthigh]);\n",
    "                firsthigh ++;\n",
    "            }\n",
    "        swap(&s[p],&s[firsthigh]);\n",
    "        \n",
    "        return(firsthigh);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/07.png)\n",
    "\n",
    "__Figure 5__: The best-case $(l)$ and worst-case $(r) $ recursion trees for quicksort . This is a recursion tree representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning steps consists of at most $ n $ swaps, it takes linear time in the number of keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with mergesort, quicksort builds a recursion tree of nested subranges of the $ n $-element array. Instead of merging, your partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, mergesort and quicksort both run in $ O(n*h) $ time, where $ h $ is the height of the recursion tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficult is that the height of the tree depends upon where the pivot element ends up in each partion. If we are luckey and _happen_ to repeatedly pick the median element as our pivot, the subproblems always half the size of the previous level. The height represents the number of times we can halve $ n $ untill we get to $ 1 $, which is $ [lg_2 n] $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get unlucky, we keep having to split the array as unequally as possible. Implies the pivot element is always the biggest or smallest element in the sub-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aftet pivot settles into position, we are left with one subproblem of size $ n - 1 $. We spent linear work and reuced the problem by one measly element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a tree height $ n - 1 $ to chop our array down to one element per level, for a worst case of $ O (n^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, quicksorts worsr case is worse than heapsort or mergesort, but average case is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Important to note that, the worst case for quicksort is $ n^2 $ because we ended up picking a terrible pivot. Imagine if you picked the smallest number of the largest number. In that case, you doing $ n - 1 $ comparisions. Then you keep picking the terrible number. On the other hand, imagine you picked a pivoit that is the medium, or the middle number. Then your splitting all the numbers less then the pivot on one half and all the numbers greater then the pivot on the other half. Now it basically becomes a problem of cutting in half each time. Halved? Well it Log time. in this case, the complexity is $ n log n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The expected case for quicksort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/08.png)\n",
    "__Figure 6__: Half the time, the pivot is close to the median element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How likely is it that a randomly selected pivot is a good one? The best possible selection for the pivot would be the median key, because exactly half of the elements would end up left and half the elements right, of the pivot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have a $ 1/n $ probability of selecting the median as pivot, which is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if a key is good enough if the pivot if it lies in the center helf of the sorted space of keys, meaning those ranked from $ n/4 $ to $ 3n/4 $ in the space of all keys to be sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such pivots are good enough since half the elements lie closer to the middle than one of the two ends. Thus on each selection, we will pick a good enough pivot with a probability of $ 1/2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worst possible enough pivot leaves the bigger half of the space partition with $ 3n/4 $ items. The height of a quicksort partition tree constructed repeatedly from the worst-possible good noug pivot is 40% taller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deepest path throug the tree passes though partions of sizes $ n, (3/4)n, (3/4)^2n, ...$ down to 1. How amny time can we multiply $ n $ by $ 3/4 $ untill it gets down to $ 1 $? Since the expected number of good splits and bad splits is the same, the bad splits only double the height of the tree, so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (3/4)^h_g n = 1 ⇒ n = (4/3)^h_g $$\n",
    "$$ h_g = log_{4/3}n $$\n",
    "$$  h = 2h_g = 2 log_{4/3}n = O(log n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since quicksort does $ O (n) $ work partitioning on each level, the average time is $ Θ (n log n) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds against $ O (n^2) $ are vanishingly small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing about quicksort runtime being $ Θ (n log n)$ is that it requires a random selection as a pivot. But if you were to sort the array first then select the pivot, you would end with a nightmare $ O (n^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we were assured randomness, we could say \"Randomized quicksort runs in $ Θ(n log n) $ time on any input with high probability\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Randomization_ is a powerful tool to improve algorithms with bad worst-case but good average-case complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Randomizing Algorithms__:\n",
    "- Random Sampling\n",
    "    - Want to get an idea of the median value of $ n $ things but dont haave either the time or space to look at them all? Select a small random sample of the input and study those for the results should be representative.\n",
    "- Random Hashing\n",
    "    - We have claimed that hashing can be used to implement dictionary operations in $ O (1) $ \"expected-time.\" However for any hash fucntion there is a given worst-case set of keys that all get hashed to the same bucket. But now suppose we randomly selected our hash function from a large family of good ones as the first step of the algorithm. We get the same type of improved guarantee that we did with randomized quicksort\n",
    "- Randomizes Search\n",
    "    - Randomization can also be used to drive search techniques such as simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuts and Bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: You are given a collection of n bolts of different widths, and $ n $ corresponding nuts. You can test whether a given nut and bolt fit together, from which you learn whether the nut is too large, too small, or an exact match for the bolt. The differences in size between pairs of nuts or bolts are too small to see by eye, so you cannot compare the sizes of two nuts or two bolts directly. You are to match each bolt to each nut. Give an $O(n^2)$ algorithm to solve the nuts and bolts problem. Then give a randomized $O(n log n) $ expected time algorithm for the same problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized quicksort is perfect for this. We ask if we can parition the nuts into bolts around a randomly selected bolt $ b$. Once we find the matching nuts to $ b $ we can use it to partition the bolts. In $ 2n- 2 $ comparisions, we partition the nuts and bolts. Then we do randomized quicksort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Quick Sort Really Quick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mergesort, heapsort and quicksort should all outperform insertion sort or selection sort on large enough instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good quicksort algorithm is 2-3 times faster than mergesort or heapsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Sort: Sorting via Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sorted names according to the first letter of the last name, we will create 26 piles or buckets of names. Then we sort by character $ ( A, B, ...)$ and then merge in the end; we are still sorting each bucket individually before we combine them. The names will be sorted when their is a single name in the bucket. The resulting algorithm is called __bucketsort__ or __distribution sort__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing is very effective when we know that the distribution of data will be roughly uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It underlies hash tables, $ kd$-trees and other practical data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down side is that the performace is terrible when the data distribution is not what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower Bounds for Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot. $ Ω(n log n) $ always exists. A sorting algorithm must behave differently during execution on each of the distinct $ n ! $ permutations of $ n $ keys. The outcome of each pairwise comparsion governs the run-time behavior of any comparison-based sorting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the set of all possible executions of such an algorithm as a tree with $ n! $ leaves. The min height tree corresponds to the fastest possible algorithm that happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ lg(n!) = Θ(n log n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Search and Related Algortihms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary search is a fast algorithm for searching in a sorted array of keys $ S $. To searchh for key $ q $, we compare $ q $ to the middle key $ S[n/2] $. If $ q $ appears before $ S[n/2] $, it must reside in the top half of $ S $; if not, it must reside in the bottom half of $ S $. By repeating the process recursively on the correct half, we locate the key in a total $ [lg n]$ comparisons, a big win over the $ n/2 $ comparisons except when using sequential search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    int binary_search(item_type s[], item_type key, int low, int high)\n",
    "    {\n",
    "        int middle; /* index of middle element */\n",
    "\n",
    "        if (low > high) return (-1); /* key not found */\n",
    "\n",
    "        middle = (low+high)/2;\n",
    "\n",
    "        if (s[middle] == key) return(middle);\n",
    "\n",
    "        if (s[middle] > key)\n",
    "            return( binary_search(s,key,low,middle-1) );\n",
    "        else\n",
    "            return(binary_search(s,key,middle+1,high) );\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using binary srarch, we can find a word in a $50,000$ to $200,000$ word dictionary in $ 20 $ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Occurances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant of binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to count the number of times a given key $ k $ occurs in a given sorted array. Because sorting groups all the copies of $ k $ into a contigous block, the problem reduces to finding the right block and then measures its size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary search routine presented above enables us to find the index of an element of the correct block $ x $ in $ O (lg n) $ time. The natural way to identify the boundaries of the block is to sequentially test elements to the left of $ x $ untill we find the first one that differs from the search key, and then repeat this search to the right of $ x $. The differences between the indices of the left and right boundaries (+1) give the count of the number of occurances of $ k $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm runs in $ O (lg n + s )$, when $ s $ is the number of occurances of the key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faster algortihm results by modifying binary search to search for the boundary of the block containing $ k $, instead of $ k $ itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we delete the equality test\n",
    "\n",
    "    if (s[middle == key) return (middle);\n",
    "    \n",
    "from the implementation above and return the index __low__ instrad of -1 on each unsuccessful search. All searches will now be unsuccessful, since there is no equality test. The search will proceede to the right half whenever the key is compared to an identical array element, eventually terminating at the right boundary. Repeating the search after reversing the direction of the binary comparison will lead us to the left boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each search takes $ O (lg n) $ time, so we cn count the occurances in log time regardless of the size of the block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Sided Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an array $ A $ consisting of a run of $ 0$'s, followed by an unbounded run of $ 1$'s and would like to identify the exact point of transition between them. Binary search on the array would provide the transition point in $ [lg n] $ tests, if we had a bounded $ n $ on the number of elements in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of a bound, we can test repeatedly at larger intervals ($ A[1], A[2], A[4], A[8], A[16], ...$) untill we find a first non-zero value. We then have a window containing the target and can proceed with binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _one-sided binary search_ finds the transition point $ p $ using at most $ 2[lg p] $ comparisons, regardless of how large the array actually is. One-sided binary search is most useful whenever we are looking for a key that lies close to our current position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square and Other Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Square root of $ n $ is the number $ r $ such that $ r^2 = n $. First observe that the square root $ n >= 1 $ must be at least 1 at most $ n $. Let $ l = 1 $ and $ r = n $. Consider the midpoint of this interval $ m = (l + r)/2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does $ m^2 $ compare to $ n $? If $ n >= m^2 $, then the square root must be greater than $ m $, so the algorithm reprats with $ l = m $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $n < m^2 $, then the square toot must be less than $ m $, so the algortihm repeats with $ r = m $. Either way we have halved the intervals using only one comparision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, after $ lg n $ rounds, we will have identified the square root within $ +- 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is called the Bisection Method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary search and its variants are the quintessential divide-and-conquer algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide and Conquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide and conquer splits the problem in halves, solves each half and then stiches the pieces back together to form a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ever merging takes less time than solving the two subproblems, we get an effcient algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mergesort and binary search are perfect examples of a divide-and-conquer algortihm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ability to analyze divide-and-conquer algortihms rests on our strenghts to solve the asymptotic of recurrences relations governing the cost of such recursive algortihms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrance relation is an equation that is define din terms of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fibonacci numbers are described by the natural functions are easily expressed by recurrences\n",
    "\n",
    "$$ F_n = F_{n-1} + F_{n-2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Any polynomial can be represented by a recurrence, such as the linear function\n",
    " \n",
    " $$ a_n = a_{n-1} + 1, a_1 = 1 → a_n = n $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any exponential can be represented by a reccurrence\n",
    "\n",
    "$$ a_n = 2a_{n-1}, a_1 = 1 → a_n = 2^{n-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lots of weird functions that cannot be described easily with conventional notation can be represented by recurrence:\n",
    "\n",
    "$$ a_n = na_{n-1}, a_a = 1 → a_n = n! $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurence relations provide a way to analyze recursive structure, such as algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide-And-Conquer Recurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide-and-conquer algortihms tend to break a given problem into some number of smalelr pieces (say $ a$), each of which is size $ n/b $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, they spend $ f(n) $ time to combine these subproblem solutions into a complete result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ T(n) $ denote the worst case time the algortihm takes to solve a problem of size $ n $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then $ T(n) $ is given by the following recurrence relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ T(n) = aT(n/b) + f(n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Examples__:\n",
    "- Sorting:\n",
    "    - The running time behavior of mergesort is governed by the currence $ T(n) = 2T(n/2) + O(n)$, since the algorithm divides the data into equal-sized halves and then spends linear time merging the halves after they are sorted. Infact, this recurrence evalutes to $ T(n) = O(n lg n) $ just as we got by our previous analysis\n",
    "- Binary Search\n",
    "    - The running time behavior of binary search is governed by the recurrence $ T(n) = T(n/2) + O(1)$, since at each step we spend constant time to reduce the problem to an instant half its size, in fact, this recurrence evaluates to $ T(n) = O(lg n) $, just as we got by the previous analysis\n",
    "- Fast Heap Construction\n",
    "    - The __bubble_down__ method of heap construction built by $ n $-element heap by constructing two $ n/2 $ element heaps and then merging them with the root in logarithmic time. This argument reduces to the reccurence relation $ T(n) = 2T(n/2) + O (lg n)$. In fact, this recurrence evaluates to $ T(n) = O(n)$, just as we got by our previous analysis\n",
    "- Matrix Multiplication\n",
    "    - The standard matrix multiplication algorithm for two $ n * n $ matrix takes $ O(n^3) $, becuase w compute the dot product of $ n $ terms for each of the $ n^2 $ elements in the product matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Divide-and-Conquer Recurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide-and-conqur recurrences of the form $ T(n) = aT(n/b) + f(n) $ are generally easy to solve because they fall into three cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if \n",
    "$$ f(n) = O(n^{log_b a  -dx})$$\n",
    "\n",
    "for some constant $ dx > 0 $, then \n",
    "\n",
    "$$ T(n) = Θ(n^log_ba) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Case 1__ hods for heap construction and matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if \n",
    "\n",
    "$$ f(n) = Θ(n^{log_ba}) $$\n",
    "\n",
    "then \n",
    "\n",
    "$$ T(n) = Θ(n^{log_ba}lgn) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Case 2__ holds for mergesort and binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if \n",
    "$$ f(n) = Ω(n^{log_b a  -dx})$$\n",
    "\n",
    "for some constant $ dx > 0 $ \n",
    "\n",
    "and if $ af(n/b) <= cf(n) $, some $ c < 1 $ then\n",
    "\n",
    "$$ T(n) = Θ(f(n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Case 3__ generally arises for clumsier algorithms, where the cost of combining the subproblems dominates everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the three cases are refered to as _master theorms_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/10.png)\n",
    "\n",
    "\n",
    "__Figrue 7__: The recursion tree resulting from decomposing each problem of size $ n $ problmes of size $ n/b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the recursion tree associated with typical $ T(n) = aT(n/b) + f(n) $ divide and conquer algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each problem of size $ n $ is decomposed into a problem of size $ n/b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each subproblem of $ k $ takes $ O(f(k)) $ time to deal with internally, between partitioning and merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time for the algorithm is the sum of these internal costs, plus the overhead of building the recursion tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The height of this tree is $ h = log_bn $ and the overhead of building the recursion tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The height of this tree is $ h = log_bn $ and the number of leaf nodes $ a^h = a^{log_bn}$, which happens to simplify to $ n^{log_ba} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three different cases of the mastr theorm correspond to three different costs which might be dominant as a function of $ a, b $ and $ f(n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__cases__:\n",
    "- 1. Too Many leaves: if the number of a leaf node outweights the sum of the internal evaluation cost, the total running time is $ O(n^{log_ba}) $\n",
    "- 2. Equal work per level: As we move down the tree, each problem gets smaller but there are more to sovle. If the sum of the internal evaluation costs at each level are qual, the total running time is cost per level $ n^{log_ba} $ times the number of levels $ log_bn $ for a total running time of $ O(n^{log_ba}lgn $\n",
    "- 3. Too expensive a root: if the internal evaluation costs grow rapidly enough with $ n $, the cost of the root evaluation may dominate, if so, the total running time is $ O(f(n)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShellSort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more effective version of Insert sort. Also called diminishing increment sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It improves on the insertion sort by breaking the original list into a number of smaller sublists, each of which is sorted using an insertion sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique way that these sublists are chosen is the key to the shell sort. Instead of breaking the list into sublists of contigous items, the shell sort uses an increment $ i $, sometimes called the gap, to create a sublist by choosing all items that ate $ i $ items apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/20.png)\n",
    "\n",
    "__Figure 8__: A shell sort with increments of three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/21.png)\n",
    "\n",
    "__Figure 9__: A shell sort after sorting each sublists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/22.png)\n",
    "\n",
    "__Figure 10__: Shellshort: A final insertion sort with incremental of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/chap4/23.png)\n",
    "\n",
    "__Figure 11__: Initial Subslits for a shell Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After increments of size 4 The list is [20, 26, 44, 17, 54, 31, 93, 55, 77]\n",
      "After increments of size 2 The list is [20, 17, 44, 26, 54, 31, 77, 55, 93]\n",
      "After increments of size 1 The list is [17, 20, 26, 31, 44, 54, 55, 77, 93]\n",
      "[17, 20, 26, 31, 44, 54, 55, 77, 93]\n"
     ]
    }
   ],
   "source": [
    "def shellSort(alist):\n",
    "    sublistcount = len(alist)//2\n",
    "    while sublistcount > 0:\n",
    "\n",
    "      for startposition in range(sublistcount):\n",
    "        gapInsertionSort(alist,startposition,sublistcount)\n",
    "\n",
    "      print(\"After increments of size\",sublistcount,\n",
    "                                   \"The list is\",alist)\n",
    "\n",
    "      sublistcount = sublistcount // 2\n",
    "\n",
    "def gapInsertionSort(alist,start,gap):\n",
    "    for i in range(start+gap,len(alist),gap):\n",
    "\n",
    "        currentvalue = alist[i]\n",
    "        position = i\n",
    "\n",
    "        while position>=gap and alist[position-gap]>currentvalue:\n",
    "            alist[position]=alist[position-gap]\n",
    "            position = position-gap\n",
    "\n",
    "        alist[position]=currentvalue\n",
    "\n",
    "alist = [54,26,93,17,77,31,44,55,20]\n",
    "shellSort(alist)\n",
    "print(alist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell short tends to fall between $ O(n) $ and  $ O (n^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redix Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuickSort in Python\n",
    "https://www.educative.io/edpresso/how-to-implement-quicksort-in-python\n",
    "\n",
    "https://www.youtube.com/watch?v=uXBnyYuwPe8\n",
    "\n",
    "https://levelup.gitconnected.com/a-sort-of-all-sorting-algorithms-506cbc76d47"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
